{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pyfredapi as pf\n",
    "import re\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from itertools import combinations, chain\n",
    "\n",
    "# Suppress specific FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from prince import FAMD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, load your api key from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# get my FRED_API_KEY from my local environment\n",
    "api_key = os.environ['FRED_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the time frame analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '1990-01-01'\n",
    "# end with the current date\n",
    "end = pd.Timestamp.today().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data not found on FRED\n",
    "\n",
    "Business confidence index (BCI) - https://data.oecd.org/leadind/business-confidence-index-bci.htm\n",
    "\n",
    "Consumer confidence index (CCI) - https://data.oecd.org/leadind/consumer-confidence-index-cci.htm#indicator-chart\n",
    "\n",
    "Composite leading indicator (CLI) - https://data.oecd.org/leadind/composite-leading-indicator-cli.htm#indicator-chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_csvs = ['INDEX_DATA/BCI.csv', 'INDEX_DATA/CCI.csv', 'INDEX_DATA/CLI.csv']\n",
    "\n",
    "index_dfs = [pd.read_csv(index_csv) for index_csv in index_csvs]\n",
    "cleaned_dfs = []\n",
    "# stack the dataframes\n",
    "for df in index_dfs:\n",
    "    indicator = df['INDICATOR'].iloc[0]\n",
    "    df = df.loc[df['LOCATION'] == 'USA', ['TIME', 'Value']].rename(columns={'Value': 'value_' + indicator, 'TIME': 'date'})\n",
    "    # set the index to the time column as a datetime object\n",
    "    df.set_index(pd.to_datetime(df['date']), inplace=True)\n",
    "    # drop the old time column\n",
    "    df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "    # drop all rows with index before string 'start'\n",
    "    df = df.loc[df.index >= start]\n",
    "\n",
    "    cleaned_dfs.append(df)\n",
    "\n",
    "# merge the cleaned_dfs\n",
    "index_df = pd.concat(cleaned_dfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are the features we will use to predict\n",
    "\n",
    "The string in the parentheses is used by the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources = \"\"\"\n",
    "(FEDFUNDS)\n",
    "Consumer Price Index for All Urban Consumers: All Items in U.S. City Average (CPIAUCSL)\n",
    "Sticky Price Consumer Price Index less Food and Energy (CORESTICKM159SFRBATL)\n",
    "Sticky Price Consumer Price Index less Food, Energy, and Shelter (CRESTKCPIXSLTRM159SFRBATL)\n",
    "Producer Price Index by Commodity: All Commodities (PPIACO)\n",
    "Personal Consumption Expenditures (PCE) \n",
    "Total Nonfarm Private Payroll Employment (ADPWNUSNERSA)\n",
    "Quarterly Financial Report: U.S. Corporations: All Information: Total Cash on Hand and in U.S. Banks  (QFRTCASHINFUSNO) - only to 2009 \n",
    "Unemployment Rate (UNRATE)\n",
    "Noncyclical Rate of Unemployment (NROU)\n",
    "Unemployment Rate - Women (LNS14000002)\n",
    "Job Openings: Total Nonfarm (JTSJOL) - begins 2000\n",
    "Layoffs and Discharges: Total Nonfarm (JTSLDL) - since 2000 Gross Domestic Product (GDP)\n",
    "Real Gross Domestic Product (GDPC1)\n",
    "Real gross domestic product per capita (A939RX0Q048SBEA)\n",
    "Gross Domestic Product: Implicit Price Deflator (A191RI1Q225SBEA)\n",
    "National Accounts: National Accounts Deflators: Gross Domestic Product: GDP Deflator for United States (USAGDPDEFQISMEI)\n",
    "Advance Retail Sales: Retail Trade and Food Services (RSAFS) - 1992 \n",
    "University of Michigan: Consumer Sentiment (UMCSENT)\n",
    "New Privately-Owned Housing Units Started: Total Units (HOUST)\n",
    "New Privately-Owned Housing Units Started: Single-Family Units (HOUST1F)\n",
    "Total Business Sales (TOTBUSSMSA) -1992\n",
    "Nonfinancial Corporate Business; Inventories Including IVA, Market Value Levels (BOGZ1LM105020005Q)\n",
    "Corporate Profits After Tax -without IVA and CCAdj- (CP)\n",
    " National income: Corporate profits before tax -without IVA and CCAdj- (A053RC1Q027SBEA)\n",
    "Money Market Funds; Total Financial Assets, Level (MMMFFAQ027S)\n",
    "Stock Market Capitalization to GDP for United States (DDDM01USA156NWDB)\n",
    "Interest Rates: Long-Term Government Bond Yields: 10-Year: Main -Including Benchmark- for United States (IRLTLT01USM156N)\n",
    "Nominal Broad U.S. Dollar Index (DTWEXBGS) - 2006\n",
    "Leading Indicators OECD: Leading indicators: CLI: Amplitude adjusted for OECD - Total (OECDLOLITOAASTSAM)\n",
    "Trade Balance: Goods and Services, Balance of Payments Basis (BOPGSTB) - 1992\n",
    "M1 (WM1NS)\n",
    "Velocity of M1 Money Stock (M1V)\n",
    "M2 (WM2NS)\n",
    "Personal consumption expenditures: Services: Gambling (DGAMRC1A027NBEA)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# Regular expression pattern to match text within parentheses\n",
    "pattern = r\"\\((.*?)\\)\"\n",
    "\n",
    "# Find all matches and store them in a list\n",
    "matches = re.findall(pattern, data_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This cell is where the api is called and stored in Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_parameters = {\n",
    "    \"observation_start\": start,\n",
    "    \"observation_end\": end,\n",
    "}\n",
    "for match in matches:\n",
    "    series_id = match\n",
    "    try:\n",
    "        df = pf.get_series(series_id=series_id, **extra_parameters)[['date', 'value']]\n",
    "        # add the match to the end of the value column name\n",
    "        df.rename(columns={'value': f'value_{match}'}, inplace=True)\n",
    "        # turn the date column into a datetime object\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # set the date column as the index\n",
    "        df.set_index('date', inplace=True)\n",
    "        dataframes[series_id] = df\n",
    "\n",
    "    except:\n",
    "        print(f\"Failed to get {series_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_dataframes(dataframes, start_date):\n",
    "\n",
    "    \"\"\"\n",
    "    Impute missing values and align the start date of each dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    aligned_dfs = []\n",
    "\n",
    "    for df in dataframes:\n",
    "        # Ensure the index is a DateTimeIndex\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "        # Resample to monthly frequency\n",
    "        # Use 'mean' for downsampling and 'ffill' for upsampling\n",
    "        resampled_df = df.resample('M').mean().ffill()\n",
    "\n",
    "        # Align start date to start, filling missing values with NaN\n",
    "        aligned_df = resampled_df.reindex(pd.date_range(start_date, resampled_df.index.max(), freq='M'), fill_value=np.nan)\n",
    "        \n",
    "        aligned_dfs.append(aligned_df)\n",
    "\n",
    "    return aligned_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, combine the FRED and non-FRED dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = list(dataframes.values()) + [index_df]\n",
    "\n",
    "# Align all DataFrames to the same frequency and start date\n",
    "aligned_dataframes = align_dataframes(dataframes_list, start)\n",
    "\n",
    "# Concatenate all DataFrames along the column axis\n",
    "combined_df = pd.concat(aligned_dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we split the features and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = combined_df.drop('value_FEDFUNDS', axis=1)\n",
    "label = combined_df['value_FEDFUNDS']\n",
    "\n",
    "# Filling missing values with the mean (change this based on your data's context)\n",
    "features_filled = features.fillna(features.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split_date: used to segment the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_collinear_combo(combo, collinear_pairs):\n",
    "    for pair in combinations(combo, 2):\n",
    "        if frozenset(pair) in collinear_pairs:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'features' is your DataFrame\n",
    "# corr_matrix = features.corr().abs()\n",
    "\n",
    "# # Threshold for considering correlation as 'high'\n",
    "# threshold = 0.8\n",
    "\n",
    "# # Grouping collinear features\n",
    "# collinear_groups = []\n",
    "# visited = set()\n",
    "\n",
    "# for col in corr_matrix.columns:\n",
    "#     if col not in visited:\n",
    "#         collinear_set = set([col])\n",
    "#         for index in corr_matrix.index[corr_matrix[col] > threshold]:\n",
    "#             if col != index:\n",
    "#                 collinear_set.add(index)\n",
    "#                 visited.add(index)\n",
    "#         collinear_groups.append(collinear_set)\n",
    "\n",
    "# # Selecting one representative from each collinear group\n",
    "# representatives = [list(group)[0] for group in collinear_groups]\n",
    "\n",
    "# # Generate all combinations of the representatives\n",
    "# combos = []\n",
    "# for i in range(1, len(representatives) + 1):\n",
    "#     combos += list(combinations(representatives, i))\n",
    "\n",
    "# combos = list(set(combos))\n",
    "\n",
    "# print(len(combos))\n",
    "# label_binary = label.diff().dropna()\n",
    "# split_date = int(len(label_binary) * 0.8)\n",
    "# label_binary = np.where(label_binary > 0, 1, 0)\n",
    "\n",
    "\n",
    "# best_combos = []\n",
    "# best_combo = None\n",
    "# best_acc = 0\n",
    "\n",
    "# for combo in combos:\n",
    "#     combo = list(set(combo))\n",
    "#     features_adjusted = features[list(combo)].iloc[1:]\n",
    "\n",
    "#     train_features = features_adjusted[:split_date]\n",
    "#     test_features = features_adjusted[split_date:]\n",
    "#     train_label = label_binary[:split_date]\n",
    "#     test_label = label_binary[split_date:]\n",
    "\n",
    "#     # Model creation and fitting\n",
    "#     xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "#     xgb_classifier.fit(train_features, train_label)\n",
    "\n",
    "#     # Predictions and evaluation\n",
    "#     xgb_predictions = xgb_classifier.predict(test_features)\n",
    "#     acc = accuracy_score(test_label, xgb_predictions)\n",
    "    \n",
    "#     if acc < best_acc:\n",
    "#         best_acc = acc\n",
    "#         best_combo = combo\n",
    "\n",
    "#     if acc > 0.70:\n",
    "#         print(\"Combo:\", combo, \"Accuracy:\", acc)\n",
    "#         best_combos.append(combo)\n",
    "\n",
    "# print(\"Best combo:\", best_combo)\n",
    "# print(\"Best MSE:\", best_acc)\n",
    "\n",
    "# features_adjusted = features[list(best_combo)].iloc[1:]\n",
    "\n",
    "# train_features = features_adjusted[:split_date]\n",
    "# test_features = features_adjusted[split_date:]\n",
    "# train_label = label_binary[:split_date]\n",
    "# test_label = label_binary[split_date:]\n",
    "\n",
    "# # Model creation and fitting\n",
    "# xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "# xgb_classifier.fit(train_features, train_label)\n",
    "\n",
    "# # Predictions and evaluation\n",
    "# xgb_predictions = xgb_classifier.predict(test_features)\n",
    "# mse = mean_squared_error(test_label, xgb_predictions)\n",
    "\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# print(accuracy_score(test_label, xgb_predictions))\n",
    "# print(classification_report(test_label, xgb_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def aggregate_before_decision(df, n_days):\n",
    "    # Ensure the index is in datetime format\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Filter rows where decisions occur\n",
    "    decision_rows = df[df['ffr'].notna()]\n",
    "\n",
    "    # Columns to be aggregated\n",
    "    columns_to_aggregate = df.columns.difference(['ffr', 'decision', 'change'])\n",
    "\n",
    "    # List to store each aggregated period's data\n",
    "    aggregated_periods = []\n",
    "\n",
    "    for date in decision_rows.index:\n",
    "        # Define the period for aggregation\n",
    "        start_date = date - pd.Timedelta(days=n_days)\n",
    "        end_date = date - pd.Timedelta(days=1)  # Exclude the decision day\n",
    "\n",
    "        # Aggregate data for this period\n",
    "        aggregated_period = df.loc[start_date:end_date, columns_to_aggregate].mean()\n",
    "\n",
    "        # Store aggregated data with the decision day's 'ffr', 'decision', and 'change' starting from the beginning\n",
    "        aggregated_period = pd.concat([df.loc[date, ['ffr', 'change', 'decision']], aggregated_period])\n",
    "        # append to the front of the list\n",
    "        aggregated_periods.append(aggregated_period)\n",
    "\n",
    "    # Create a DataFrame from the aggregated periods\n",
    "    aggregated_data = pd.DataFrame(aggregated_periods)\n",
    "\n",
    "    return aggregated_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = aggregate_before_decision(pd.read_csv('master_data.csv'), 30)\n",
    "# Split data into features and target\n",
    "X = aggregated_df.drop(['ffr', 'decision', 'change'], axis=1)\n",
    "y = aggregated_df['decision']\n",
    "\n",
    "# combos = get_combos(X)\n",
    "\n",
    "\n",
    "\n",
    "# # # turn to -1,0,1 to 0, 1, 2\n",
    "# y = y + 1\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "# best_combos = []\n",
    "# best_combo = None\n",
    "# best_acc = 0\n",
    "\n",
    "# # for i,combo in enumerate(combos):\n",
    "# #     combo = list(set(combo))\n",
    "# #     features_adjusted = df[list(combo)].iloc[1:]\n",
    "\n",
    "# #     # Model creation and fitting\n",
    "# #     xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "# #     xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# #     # Predictions and evaluation\n",
    "# #     xgb_predictions = xgb_classifier.predict(X_test)\n",
    "# #     acc = accuracy_score(y_test, xgb_predictions)\n",
    "    \n",
    "# #     if acc > best_acc:\n",
    "# #         best_acc = acc\n",
    "# #         best_combo = combo\n",
    "# #         print(\"New best combo:\", best_combo, \"Accuracy:\", best_acc)\n",
    "\n",
    "# #     if acc > 0.60:\n",
    "# #         print(\"Combo:\", combo, \"Accuracy:\", acc)\n",
    "# #         best_combos.append(combo)\n",
    "\n",
    "# print(\"Best combo:\", best_combo)\n",
    "# print(\"Best MSE:\", best_acc)\n",
    "\n",
    "# # Classifier - can be replaced with a more suitable model for time series\n",
    "# classifier = RandomForestClassifier()\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions\n",
    "# predictions = classifier.predict(X_test)\n",
    "\n",
    "# # Evaluation\n",
    "# print(classification_report(y_test, predictions))\n",
    "\n",
    "# xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "# xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predictions and evaluation\n",
    "# xgb_predictions = xgb_classifier.predict(X_test)\n",
    "\n",
    "# print(classification_report(y_test, xgb_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'features' is your DataFrame\n",
    "# corr_matrix = features.corr().abs()\n",
    "\n",
    "# # Threshold for considering correlation as 'high'\n",
    "# threshold = 0.1\n",
    "\n",
    "# # Grouping collinear features\n",
    "# collinear_groups = []\n",
    "# already_grouped = set()\n",
    "\n",
    "# for col in corr_matrix.columns:\n",
    "#     if col not in already_grouped:\n",
    "#         collinear_set = {col}\n",
    "#         for index in corr_matrix.index[corr_matrix[col] > threshold]:\n",
    "#             if col != index and index not in already_grouped:\n",
    "#                 collinear_set.add(index)\n",
    "#                 already_grouped.add(index)\n",
    "#         if len(collinear_set) > 1:\n",
    "#             collinear_groups.append(collinear_set)\n",
    "\n",
    "# # Select one representative from each collinear group\n",
    "# representatives = [list(group)[0] for group in collinear_groups]\n",
    "\n",
    "# # Add non-collinear features to the representatives\n",
    "# non_collinear_features = [col for col in features.columns if col not in already_grouped]\n",
    "# representatives.extend(non_collinear_features)\n",
    "\n",
    "# # Create a set of all unique representatives and non-collinear features\n",
    "# unique_features = set(representatives)\n",
    "\n",
    "# # Generate combinations of varying sizes from this set\n",
    "# combos = list(chain.from_iterable(combinations(unique_features, r) for r in range(1, len(unique_features)+1)))\n",
    "\n",
    "# print(f\"Number of combinations: {len(combos)}\")\n",
    "\n",
    "# # print(combos)\n",
    "\n",
    "\n",
    "# for combo in combos:\n",
    "#     print(combo)\n",
    "\n",
    "\n",
    "#     features_adjusted = features[list(combo)].iloc[1:]\n",
    "\n",
    "#     train_features = features_adjusted[:split_date]\n",
    "#     test_features = features_adjusted[split_date:]\n",
    "#     train_label = label_binary[:split_date]\n",
    "#     test_label = label_binary[split_date:]\n",
    "#     features_adjusted = features[list(best_combo)].iloc[1:]\n",
    "\n",
    "\n",
    "#     # Model creation and fitting\n",
    "#     xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "#     xgb_classifier.fit(train_features, train_label)\n",
    "\n",
    "#     # Predictions and evaluation\n",
    "#     xgb_predictions = xgb_classifier.predict(test_features)\n",
    "#     mse = mean_squared_error(test_label, xgb_predictions)\n",
    "\n",
    "#     print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "#     print(accuracy_score(test_label, xgb_predictions))\n",
    "#     print(classification_report(test_label, xgb_predictions))\n",
    "\n",
    "#     # xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "#     # # Define the parameter grid\n",
    "#     param_grid = {\n",
    "#         'n_estimators': [50, 100, 150],\n",
    "#         'learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'max_depth': [3, 5, 7],\n",
    "#         'subsample': [0.8, 1.0],  # Fraction of samples to be used for fitting each tree\n",
    "#         'colsample_bytree': [0.8, 1.0]  # Fraction of features to be used for each tree\n",
    "#     }\n",
    "\n",
    "#     # # Grid Search with Cross-Validation\n",
    "#     # grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "#     # grid_search.fit(train_features, train_label)\n",
    "\n",
    "#     # # Best parameters\n",
    "#     # print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "#     # # Evaluate on test set with the best model\n",
    "#     # best_model = grid_search.best_estimator_\n",
    "#     # xgb_predictions = best_model.predict(test_features)\n",
    "\n",
    "#     # mse = mean_squared_error(test_label, xgb_predictions)\n",
    "#     # accuracy = accuracy_score(test_label, xgb_predictions)\n",
    "\n",
    "#     # print(\"Mean Squared Error:\", mse)\n",
    "#     # print(\"Accuracy:\", accuracy)\n",
    "#     # print(classification_report(test_label, xgb_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have the best features, we want to do a gridsearch to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep a df to be used for classifying\n",
    "# n_days is how many days prior to the announcement you want to forecast\n",
    "def forecast_n_days_prior(df, n_days = 7):\n",
    "    df_temp = df.copy()\n",
    "    df_temp.iloc[:,:3] = df_temp.iloc[:,:3].shift(-n_days)\n",
    "    return df_temp.query('ffr == ffr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_train(X, y, label_type='classification'):\n",
    "    \n",
    "    if label_type == 'classification':\n",
    "        # turn to -1,0,1 to 0, 1, 2\n",
    "        y = y + 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "        'max_depth': [10, 20, 30],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at each leaf node\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider when looking for the best split\n",
    "    }\n",
    "\n",
    "\n",
    "    # perfor grid search\n",
    "    grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # train model with best parameters\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    return best_model, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def famd_df(df, n_components=4, cat_threshold=4):\n",
    "    # Get indices of categorical columns\n",
    "    indices = []\n",
    "    for i, name in enumerate(df.columns):\n",
    "        # Assuming 'df' is your DataFrame and 'column' is the column you want to check\n",
    "        unique_count = df[name].nunique()\n",
    "        if unique_count < cat_threshold:  # Adjust the threshold as needed\n",
    "            indices.append(i)\n",
    "    \n",
    "    df_encoded = df.iloc[:, indices]\n",
    "    df_numeric = df.drop(df.columns[indices], axis=1)\n",
    "    \n",
    "    # Then we scale the numeric data\n",
    "    scaler = StandardScaler()\n",
    "    df_numeric = scaler.fit_transform(df_numeric)\n",
    "    df_numeric = pd.DataFrame(df_numeric, index=df.index)\n",
    "    \n",
    "    # The function FAMD imported from prince requires categorical data to\n",
    "    # Be of type 'object' so we convert it since it is already encoded\n",
    "    df_categorical = df_encoded.astype(str)\n",
    "    \n",
    "    df_final = pd.concat([df_numeric, df_categorical], axis=1)\n",
    "    \n",
    "    famd = FAMD(n_components=n_components)\n",
    "    \n",
    "    X_famd = famd.fit_transform(df_final)\n",
    "    \n",
    "    return X_famd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = famd_df(pd.read_csv('beta_dates/beta_data_7_60.csv', index_col=0))\n",
    "# y = pd.read_csv('labels.csv', index_col=0).values[:,0]\n",
    "# rf_train(df, y, label_type='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithms and In-Depth Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use random forests and XGBoost for training our models, using the collapsed data and using FAMD to reducethe dimensionality of the data to only 4 principle components. We use grid search to find the best hyperparameters for the models.\n",
    "\n",
    "In the end, we find that the best model is a random forest with the following hyperparameters:\n",
    "\n",
    " {'max_depth': 20,\n",
    "  'max_features': 'log2',\n",
    "  'min_samples_leaf': 4,\n",
    "  'min_samples_split': 5,\n",
    "  'n_estimators': 50})\n",
    "\n",
    "  The accuracy matrix for this model is:\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.00      0.00      0.00         5\n",
    "         1.0       0.59      0.90      0.71        30\n",
    "         2.0       0.67      0.30      0.41        20\n",
    "\n",
    "    accuracy                           0.60        55\n",
    "   macro avg       0.42      0.40      0.37        55\n",
    "weighted avg       0.56      0.60      0.54        55\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_train(X, y, label_type='classification'):\n",
    "    \n",
    "    if label_type == 'classification':\n",
    "        # turn to -1,0,1 to 0, 1, 2\n",
    "        y = y + 1\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2, random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 110],  # Reduced number of trees\n",
    "        'learning_rate': [0.01, 0.8],  # Expanded range with a lower bound\n",
    "        'max_depth': [7, 8],  # Shallower trees considering fewer components\n",
    "        'min_child_weight': [4, 5],  # Adjusted values for instance weight\n",
    "        'gamma': [0],  # Slightly expanded range for loss reduction\n",
    "        'subsample': [0.7],  # Adjusted subsample ratio\n",
    "        'colsample_bytree': [0.8, 0.9],  # Adjusted subsample ratio of columns\n",
    "        'reg_alpha': [0.1],  # Adjusted L1 regularization term\n",
    "        'reg_lambda': [0.1]  # Adjusted L2 regularization term\n",
    "    }\n",
    "    \n",
    "\n",
    "    # perfor grid search\n",
    "    grid_search = GridSearchCV(estimator=xgb.XGBClassifier(objective='multi:softmax', random_state=42), param_grid=param_grid, cv=3, scoring='accuracy', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # train model with best parameters\n",
    "    xgb_classifier = xgb.XGBClassifier(**grid_search.best_params_, objective='multi:softmax', random_state=42)\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = xgb_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    return xgb_classifier, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/prince/pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.4177863742936748, -1.1512838683202964, -0.9075441386711521, -0.8224133002108104, -1.1677484162422844, -0.3692744729379982]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "/opt/homebrew/lib/python3.10/site-packages/prince/pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.4177863742936748, -1.1512838683202964, -0.9075441386711521, -0.8224133002108104, -1.1677484162422844, -0.3692744729379982]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/matthewmella/how-interest-ing/matt-version.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m famd_df(pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mbeta_dates/beta_data_7_60.csv\u001b[39m\u001b[39m'\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mlabels.csv\u001b[39m\u001b[39m'\u001b[39m, index_col\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mvalues[:,\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m xgboost_train(df, y, label_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mclassification\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/matthewmella/how-interest-ing/matt-version.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# perfor grid search\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mxgb\u001b[39m.\u001b[39mXGBClassifier(objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmulti:softmax\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m), param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# train model with best parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/matthewmella/how-interest-ing/matt-version.ipynb#Y165sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m xgb_classifier \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgrid_search\u001b[39m.\u001b[39mbest_params_, objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmulti:softmax\u001b[39m\u001b[39m'\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/sklearn.py:1515\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1487\u001b[0m (\n\u001b[1;32m   1488\u001b[0m     model,\n\u001b[1;32m   1489\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1494\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1495\u001b[0m )\n\u001b[1;32m   1496\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1497\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1498\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1512\u001b[0m     feature_types\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_types,\n\u001b[1;32m   1513\u001b[0m )\n\u001b[0;32m-> 1515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1516\u001b[0m     params,\n\u001b[1;32m   1517\u001b[0m     train_dmatrix,\n\u001b[1;32m   1518\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1519\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1520\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1521\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1522\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1523\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1524\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1525\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1526\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1527\u001b[0m )\n\u001b[1;32m   1529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1530\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/xgboost/core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     _check_call(\n\u001b[0;32m-> 2050\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[1;32m   2051\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[1;32m   2052\u001b[0m         )\n\u001b[1;32m   2053\u001b[0m     )\n\u001b[1;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = famd_df(pd.read_csv('beta_dates/beta_data_7_60.csv', index_col=0))\n",
    "y = pd.read_csv('labels.csv', index_col=0).values[:,0]\n",
    "xgboost_train(df, y, label_type='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/prince/pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.4177863742936748, -1.1512838683202964, -0.9075441386711521, -0.8224133002108104, -1.1677484162422844, -0.3692744729379982]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "/opt/homebrew/lib/python3.10/site-packages/prince/pca.py:175: PerformanceWarning: Concatenating sparse arrays with multiple fill values: '[-0.4177863742936748, -1.1512838683202964, -0.9075441386711521, -0.8224133002108104, -1.1677484162422844, -0.3692744729379982]'. Picking the first and converting the rest.\n",
      "  X = self.scaler_.transform(X.to_numpy())\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "X = famd_df(pd.read_csv('beta_dates/beta_data_7_60.csv', index_col=0), n_components=3, cat_threshold=4)\n",
    "y = pd.read_csv('true_labels.csv', index_col=0).values[:,0]\n",
    "\n",
    "y = y + 1\n",
    "classification_list = []\n",
    "for i in range(1000):\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.2)\n",
    "\n",
    "    params =  {'colsample_bytree': 0.8,\n",
    "    'gamma': 0,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 4,\n",
    "    'n_estimators': 110,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'subsample': 0.7}\n",
    "\n",
    "    # train model with best parameters\n",
    "    xgb_classifier = xgb.XGBClassifier(**params, objective='multi:softmax', random_state=42)\n",
    "    xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    predictions = xgb_classifier.predict(X_test)\n",
    "\n",
    "    # add to list\n",
    "    classification_list.append(classification_report(y_test, predictions, output_dict=True))\n",
    "\n",
    "# Evaluation\n",
    "# print(classification_report(y_test, predictions))\n",
    "\n",
    "# print('y_test:', y_test)\n",
    "# print('predictions:', predictions)\n",
    "\n",
    "\n",
    "# feature_importances = xgb_classifier.feature_importances_\n",
    "# plt.barh(range(len(feature_importances)), feature_importances)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.7456545454545469\n"
     ]
    }
   ],
   "source": [
    "# get the average accuracy\n",
    "avg_acc = 0\n",
    "for report in classification_list:\n",
    "    avg_acc += report['accuracy']\n",
    "avg_acc /= len(classification_list)\n",
    "print('Average accuracy:', avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # properly read in data\n",
    "# df = pd.read_csv('master_data.csv', index_col=0)\n",
    "\n",
    "# days_prior = forecast_n_days_prior(df, 7)\n",
    "\n",
    "# # We split the df into numeric and categorical data\n",
    "# df_numeric = days_prior.drop(['decision', 'recess', 'fed_party', 'potus_party'], axis=1)\n",
    "# df_encoded = days_prior[['decision', 'recess', 'fed_party', 'potus_party']]\n",
    "\n",
    "# # Then we scale the numeric data\n",
    "# scaler = StandardScaler()\n",
    "# df_numeric = scaler.fit_transform(df_numeric)\n",
    "# df_numeric = pd.DataFrame(df_numeric, index=days_prior.index)\n",
    "\n",
    "# # The function FAMD imported from prince requires categorical data to\n",
    "# # Be of type 'object' so we convert it since it is already encoded\n",
    "# df_categorical = df_encoded.astype(str)\n",
    "# days_prior_mixed = pd.concat([df_numeric, df_categorical], axis=1)\n",
    "\n",
    "# X = days_prior_mixed.drop(['decision'], axis=1)\n",
    "\n",
    "# famd = FAMD(n_components=3)\n",
    "\n",
    "# X_famd = famd.fit_transform(X).values\n",
    "# y = days_prior_mixed['decision'].values\n",
    "# # make y ints\n",
    "# y = y.astype(float).astype(int)\n",
    "# xgboost_train(X_famd, y, label_type='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key ideas: \n",
    "How to handle time series - No CV, regression, issues\n",
    "\n",
    "b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As a priliminary approach, we decided to pull around 35 feature columns using the Federal Reserve Economic Data (FRED),\n",
    "the run a correlation matrix to find collinear features. We then grouped the collinear features and selected one representative.\n",
    "We then generated all combinations of the representatives and ran a grid search to find the best combination of features, with the \n",
    "model being an XGBoost classifier. However, an issue with this approach was the reality of working with time series data. We coul\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('master_data.csv', index_col=0)\n",
    "# aggregated_df = aggregate_before_decision(df, n_days=30)  # Aggregate 30 days before each decision\n",
    "\n",
    "# # Split data into features and target\n",
    "# X = aggregated_df.drop(['ffr', 'decision', 'change'], axis=1)\n",
    "# y = aggregated_df['decision']\n",
    "\n",
    "# combos = get_combos(X)\n",
    "\n",
    "# # # turn to -1,0,1 to 0, 1, 2\n",
    "# y = y + 1\n",
    "\n",
    "# # Split data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300],  # Number of boosting rounds\n",
    "#     'learning_rate': [0.05, 0.1],  # Step size shrinkage\n",
    "#     'max_depth': [5, 7],  # Maximum tree depth\n",
    "#     'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight needed in a child\n",
    "#     'gamma': [0, 0.1, 0.2],  # Minimum loss reduction required for a split\n",
    "#     'subsample': [0.7, 0.8, 0.9],  # Subsample ratio of the training instance\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9],  # Subsample ratio of columns when constructing each tree\n",
    "#     'reg_alpha': [0, 0.1, 0.5],  # L1 regularization term\n",
    "#     'reg_lambda': [1, 1.5, 2]  # L2 regularization term\n",
    "# }\n",
    "\n",
    "\n",
    "# # Manual Grid Search\n",
    "# best_params = None\n",
    "# best_accuracy = 0\n",
    "# best_params_list = []\n",
    "\n",
    "# i = 0\n",
    "# for n_estimators in param_grid['n_estimators']:\n",
    "#     for learning_rate in param_grid['learning_rate']:\n",
    "#         for max_depth in param_grid['max_depth']:\n",
    "#             for min_child_weight in param_grid['min_child_weight']:\n",
    "#                 for gamma in param_grid['gamma']:\n",
    "#                     for subsample in param_grid['subsample']:\n",
    "#                         for colsample_bytree in param_grid['colsample_bytree']:\n",
    "#                             for reg_alpha in param_grid['reg_alpha']:\n",
    "#                                 for reg_lambda in param_grid['reg_lambda']:\n",
    "#                                     # Create and fit the model\n",
    "#                                     xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', n_estimators=n_estimators,\n",
    "#                                                                           learning_rate=learning_rate, max_depth=max_depth,\n",
    "#                                                                           min_child_weight=min_child_weight, gamma=gamma,\n",
    "#                                                                           subsample=subsample, colsample_bytree=colsample_bytree,\n",
    "#                                                                           reg_alpha=reg_alpha, reg_lambda=reg_lambda, random_state=42)\n",
    "#                                     xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "#                                     # Evaluate on the validation set\n",
    "#                                     val_predictions = xgb_classifier.predict(X_test)\n",
    "#                                     accuracy = accuracy_score(y_test, val_predictions)\n",
    "\n",
    "#                                     # Update best params\n",
    "#                                     if accuracy > best_accuracy:\n",
    "#                                         best_accuracy = accuracy\n",
    "#                                         best_params = {'n_estimators': n_estimators, 'learning_rate': learning_rate,\n",
    "#                                                          'max_depth': max_depth, 'min_child_weight': min_child_weight,\n",
    "#                                                          'gamma': gamma, 'subsample': subsample,\n",
    "#                                                          'colsample_bytree': colsample_bytree,\n",
    "#                                                          'reg_alpha': reg_alpha, 'reg_lambda': reg_lambda}\n",
    "                                        \n",
    "#                                     if accuracy > 0.58:\n",
    "#                                         best_params_list.append({'n_estimators': n_estimators, 'learning_rate': learning_rate,\n",
    "#                                                          'max_depth': max_depth, 'min_child_weight': min_child_weight,\n",
    "#                                                          'gamma': gamma, 'subsample': subsample,\n",
    "#                                                          'colsample_bytree': colsample_bytree,\n",
    "#                                                          'reg_alpha': reg_alpha, 'reg_lambda': reg_lambda})\n",
    "#                                         print(\"Accuracy:\", accuracy)\n",
    "#                                         print(classification_report(y_test, val_predictions))\n",
    "#                                     print(i)\n",
    "#                                     i += 1\n",
    "                                        \n",
    "# print(\"Best parameters:\", best_params)\n",
    "# print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, train the best classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 5, 'min_child_weight': 1, 'gamma': 0, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 0.5, 'reg_lambda': 2}\n",
      "Mean Squared Error: 0.18292682926829268\n",
      "0.8170731707317073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78        34\n",
      "           1       0.85      0.83      0.84        48\n",
      "\n",
      "    accuracy                           0.82        82\n",
      "   macro avg       0.81      0.81      0.81        82\n",
      "weighted avg       0.82      0.82      0.82        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split_date = int(len(label_binary) * 0.8)\n",
    "# features_adjusted = features[list(best_combo)].iloc[1:]\n",
    "# train_features = features_adjusted[:split_date]\n",
    "# test_features = features_adjusted[split_date:]\n",
    "# train_label = label_binary[:split_date]\n",
    "# test_label = label_binary[split_date:]\n",
    "# features_adjusted = features[list(best_combo)].iloc[1:]\n",
    "# # train using the best parameters\n",
    "# print(best_params)\n",
    "# xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', **best_params, random_state=42)\n",
    "# xgb_classifier.fit(train_features, train_label)\n",
    "\n",
    "# # Predictions and evaluation\n",
    "# xgb_predictions = xgb_classifier.predict(test_features)\n",
    "# mse = mean_squared_error(test_label, xgb_predictions)\n",
    "\n",
    "# print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# print(accuracy_score(test_label, xgb_predictions))\n",
    "# print(classification_report(test_label, xgb_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming 'features' is your DataFrame\n",
    "# corr_matrix = features.corr().abs()\n",
    "\n",
    "# # Threshold for considering correlation as 'high'\n",
    "# threshold = 0.8\n",
    "\n",
    "# # Grouping collinear features\n",
    "# collinear_groups = []\n",
    "# already_grouped = set()\n",
    "\n",
    "# for col in corr_matrix.columns:\n",
    "#     if col not in already_grouped:\n",
    "#         collinear_set = {col}\n",
    "#         for index in corr_matrix.index[corr_matrix[col] > threshold]:\n",
    "#             if col != index and index not in already_grouped:\n",
    "#                 collinear_set.add(index)\n",
    "#                 already_grouped.add(index)\n",
    "#         if len(collinear_set) > 1:\n",
    "#             collinear_groups.append(collinear_set)\n",
    "\n",
    "#             # Assuming 'label' is your target variable\n",
    "# label_corr = features.corrwith(label).abs()\n",
    "\n",
    "# # Filter out non-collinear features that are highly correlated with the label\n",
    "# non_collinear_features = [col for col in features.columns if col not in already_grouped and label_corr[col] <= threshold]\n",
    "\n",
    "# # Generate all possible combinations of non-collinear features\n",
    "# non_collinear_combos = []\n",
    "# for i in range(1, len(non_collinear_features) + 1):\n",
    "#     non_collinear_combos += list(combinations(non_collinear_features, i))\n",
    "\n",
    "# # Generate all combinations where each feature in a collinear group can be a representative\n",
    "# all_possible_collinear_combos = list(product(*collinear_groups))\n",
    "\n",
    "# # Combine collinear and non-collinear combinations\n",
    "# final_combos = []\n",
    "# for collinear_combo in all_possible_collinear_combos:\n",
    "#     for non_collinear_combo in non_collinear_combos:\n",
    "#         final_combos.append(collinear_combo + non_collinear_combo)\n",
    "\n",
    "# # remove duplicates from final_combos\n",
    "# final_combos = list(set(final_combos))\n",
    "# print(len(final_combos))\n",
    "\n",
    "\n",
    "\n",
    "# label_binary = label.diff().dropna()\n",
    "# split_date = int(len(label_binary) * 0.8)\n",
    "# label_binary = np.where(label_binary > 0, 1, 0)\n",
    "\n",
    "# best_combo = None\n",
    "# best_MSE = np.inf\n",
    "\n",
    "# for i, combo in enumerate(final_combos):\n",
    "#     # remove duplicate elements in combo\n",
    "#     combo = list(set(combo))\n",
    "#     print(i)\n",
    "#     features_adjusted = features[list(combo)].iloc[1:]\n",
    "\n",
    "#     train_features = features_adjusted[:split_date]\n",
    "#     test_features = features_adjusted[split_date:]\n",
    "#     train_label = label_binary[:split_date]\n",
    "#     test_label = label_binary[split_date:]\n",
    "\n",
    "#     # Model creation and fitting\n",
    "#     xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "#     xgb_classifier.fit(train_features, train_label)\n",
    "\n",
    "#     # Predictions and evaluation\n",
    "#     xgb_predictions = xgb_classifier.predict(test_features)\n",
    "#     mse = mean_squared_error(test_label, xgb_predictions)\n",
    "    \n",
    "#     if mse < best_MSE:\n",
    "#         best_MSE = mse\n",
    "#         best_combo = combo\n",
    "\n",
    "# print(best_combo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a binary label for increase (1) or decrease (0) in interest rates\n",
    "# label_binary = label.diff().dropna()\n",
    "# label_binary = np.where(label_binary > 0, 1, 0)\n",
    "\n",
    "# # Shift the binary label forward by one period\n",
    "# for i in range(1, 13):\n",
    "#     label_shifted = np.roll(label_binary, -i)\n",
    "\n",
    "#     features_aligned = features.iloc[1:-1]\n",
    "#     label_aligned = label_shifted[:-1]\n",
    "\n",
    "\n",
    "#     split_index = int(len(label_aligned) * 0.8)\n",
    "\n",
    "#     train_features = features_aligned[:split_index]\n",
    "#     test_features = features_aligned[split_index:]\n",
    "#     train_label = label_aligned[:split_index]\n",
    "#     test_label = label_aligned[split_index:]\n",
    "\n",
    "\n",
    "#     xgb_classifier = xgb.XGBClassifier(objective='binary:logistic', learning_rate=0.1, max_depth=5, random_state=42)\n",
    "#     xgb_classifier.fit(train_features, train_label)\n",
    "\n",
    "#     xgb_predictions = xgb_classifier.predict(test_features)\n",
    "#     accuracy = accuracy_score(test_label, xgb_predictions)\n",
    "\n",
    "#     print(\"Accuracy:\", accuracy)\n",
    "#     # print(classification_report(test_label, xgb_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get tags\n",
    "# request_url = f'https://api.stlouisfed.org/fred/tags'\n",
    "\n",
    "# params = {\n",
    "#     'api_key': api_key,\n",
    "#     'file_type': 'json',\n",
    "#     'limit': 1000,\n",
    "#     'order_by': 'popularity',\n",
    "#     # 'offset': 0,\n",
    "# }\n",
    "# tags_response = requests.get(request_url, params=params)\n",
    "# # response_json = tags_response.json()\n",
    "# # tags_df = pd.DataFrame(response_json['tags'])\n",
    "# # display(tags_df)\n",
    "# # print(tags_df['name'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response_json = tags_response.json()\n",
    "# tags_df = pd.DataFrame(response_json['tags'])\n",
    "# display(tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request_url = f'https://api.stlouisfed.org/fred/releases'\n",
    "\n",
    "# params = {\n",
    "#     'api_key': api_key,\n",
    "#     'file_type': 'json',\n",
    "#     'limit': 1000,\n",
    "#     # 'offset': 10,\n",
    "# }\n",
    "# response = requests.get(request_url, params=params)\n",
    "# # # print the 'name' for each Release\n",
    "# # for release in response.json()['releases']:\n",
    "# #     print(release['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
