{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from prince import FAMD\n",
    "from scipy import sparse\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df, col_split):\n",
    "    \"\"\"\n",
    "    Prepares the data for the model by standardizing the continuous features, \n",
    "    converting the categorical features to strings, \n",
    "    and converting sparse columns to a dense format for FAMD.\n",
    "    \"\"\"\n",
    "    categorical_cols = df.columns[:col_split]\n",
    "\n",
    "    # converting the categorical features to strings\n",
    "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(data_address='beta_dates/beta_data_7_60.csv', label_address='beta_dates/true_labels.csv'):\n",
    "    df = pd.read_csv(data_address, index_col=0)\n",
    "    y = pd.read_csv(label_address, index_col=0).values[:,0]\n",
    "    y = y + 1\n",
    "    X = encoder(df, 4)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def load_data_cv(data_address='beta_dates/beta_data_7_60.csv', label_address='beta_dates/true_labels.csv'):\n",
    "    df = pd.read_csv(data_address, index_col=0)\n",
    "    y = pd.read_csv(label_address, index_col=0).values[:,0]\n",
    "    y = y + 1\n",
    "    X = encoder(df, 4)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'rf__max_depth': 3, 'rf__max_features': 0.6639830714160453, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 8, 'rf__n_estimators': 143}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from prince import FAMD  # Ensure prince is installed\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# import numpy as np\n",
    "\n",
    "# # Number of splits for K-Fold Cross-Validation\n",
    "# n_splits = 5\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# # Load your data\n",
    "# X, y = load_data_cv('beta_dates/beta_data_2_42.csv', 'beta_dates/true_labels.csv')\n",
    "\n",
    "# # Perform FAMD on the dataset\n",
    "# famd = FAMD(n_components=8)\n",
    "# famd.fit(X)\n",
    "# X = famd.transform(X)\n",
    "\n",
    "# # Create a pipeline with the best parameters\n",
    "# pipe = Pipeline([\n",
    "#     ('scaler1', MaxAbsScaler()),\n",
    "#     ('scaler2', MaxAbsScaler()),  # Consider if you really need two MaxAbsScalers\n",
    "#     ('standard_scaler', StandardScaler()),\n",
    "#     ('classifier', RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=0.55, min_samples_leaf=6, min_samples_split=17, n_estimators=100))\n",
    "# ])\n",
    "\n",
    "# # Perform K-Fold Cross-Validation\n",
    "# accuracy_scores = []\n",
    "# for train_index, test_index in skf.split(X, y):\n",
    "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#     # Fit the pipeline to the training data\n",
    "#     pipe.fit(X_train, y_train)\n",
    "\n",
    "#     # Make predictions on the test set\n",
    "#     predictions = pipe.predict(X_test)\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     print(classification_report(y_test, predictions))\n",
    "#     print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "#     accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# print(\"Average Accuracy:\", np.mean(accuracy_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.29      0.44         7\n",
      "         1.0       0.81      1.00      0.89        38\n",
      "         2.0       1.00      0.60      0.75        10\n",
      "\n",
      "    accuracy                           0.84        55\n",
      "   macro avg       0.94      0.63      0.70        55\n",
      "weighted avg       0.87      0.84      0.81        55\n",
      "\n",
      "Accuracy: 0.8363636363636363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.33      0.44         6\n",
      "         1.0       0.79      0.97      0.87        38\n",
      "         2.0       1.00      0.45      0.62        11\n",
      "\n",
      "    accuracy                           0.80        55\n",
      "   macro avg       0.82      0.59      0.65        55\n",
      "weighted avg       0.82      0.80      0.77        55\n",
      "\n",
      "Accuracy: 0.8\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.50      0.55         6\n",
      "         1.0       0.78      0.92      0.84        38\n",
      "         2.0       0.80      0.36      0.50        11\n",
      "\n",
      "    accuracy                           0.76        55\n",
      "   macro avg       0.73      0.59      0.63        55\n",
      "weighted avg       0.76      0.76      0.74        55\n",
      "\n",
      "Accuracy: 0.7636363636363637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.14      0.25         7\n",
      "         1.0       0.77      0.97      0.86        37\n",
      "         2.0       0.86      0.55      0.67        11\n",
      "\n",
      "    accuracy                           0.78        55\n",
      "   macro avg       0.87      0.55      0.59        55\n",
      "weighted avg       0.81      0.78      0.74        55\n",
      "\n",
      "Accuracy: 0.7818181818181819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.29      0.44         7\n",
      "         1.0       0.77      0.92      0.84        37\n",
      "         2.0       0.62      0.50      0.56        10\n",
      "\n",
      "    accuracy                           0.76        54\n",
      "   macro avg       0.80      0.57      0.61        54\n",
      "weighted avg       0.77      0.76      0.74        54\n",
      "\n",
      "Accuracy: 0.7592592592592593\n",
      "Average Accuracy: 0.7882154882154883\n"
     ]
    }
   ],
   "source": [
    "# Number of splits for K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "X, y = load_data_cv('beta_dates/beta_data_2_42.csv', 'beta_dates/true_labels.csv')\n",
    "\n",
    "famd = FAMD(n_components=12)\n",
    "famd.fit(X)\n",
    "X = famd.transform(X)\n",
    "\n",
    "# create a pipeline with the best parameters\n",
    "pipe = Pipeline([\n",
    "    ('rf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "params = {'rf__max_depth': 4, 'rf__max_features': 0.6407209567554418, 'rf__min_samples_leaf': 6, 'rf__min_samples_split': 17, 'rf__n_estimators': 195}\n",
    "params = {'rf__max_depth': 3, 'rf__max_features': 0.6639830714160453, 'rf__min_samples_leaf': 2, 'rf__min_samples_split': 8, 'rf__n_estimators': 143}\n",
    "pipe.set_params(**params)\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "\n",
    "accuracy_scores = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the pipeline to the training data\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipe.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(classification_report(y_test, predictions))\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "print(\"Average Accuracy:\", np.mean(accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Average Accuracy: 0.8065993265993265\n",
      "Iteration 2, Average Accuracy: 0.7991919191919192\n",
      "Iteration 3, Average Accuracy: 0.7992592592592592\n",
      "Iteration 4, Average Accuracy: 0.7993265993265993\n",
      "Iteration 5, Average Accuracy: 0.7882828282828283\n",
      "Iteration 6, Average Accuracy: 0.799057239057239\n",
      "Iteration 7, Average Accuracy: 0.7921885521885523\n",
      "Iteration 8, Average Accuracy: 0.7811447811447811\n",
      "Iteration 9, Average Accuracy: 0.7842424242424243\n",
      "Iteration 10, Average Accuracy: 0.7808080808080808\n",
      "Overall Average Accuracy: 0.7930101010101009\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy_scores = []\n",
    "# Load your data\n",
    "X, y = load_data_cv('beta_dates/beta_data_2_42.csv', 'beta_dates/true_labels.csv')\n",
    "\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "for iteration in range(10):\n",
    "\n",
    "\n",
    "    # Define the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('rf', RandomForestClassifier()),\n",
    "    ])\n",
    "\n",
    "    # Set pipeline parameters\n",
    "    params = {'rf__max_depth': 4, 'rf__max_features': 0.65, 'rf__min_samples_leaf': 6, 'rf__min_samples_split': 17, 'rf__n_estimators': 200}\n",
    "    pipe.set_params(**params)\n",
    "\n",
    "    # Initialize Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    # List to store accuracy scores for this iteration\n",
    "    accuracy_scores = []\n",
    "\n",
    "    # Perform K-Fold Cross-Validation\n",
    "    for train_index, test_index in skf.split(X_transformed, y):\n",
    "        X_train, X_test = X_transformed.iloc[train_index], X_transformed.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit the pipeline and make predictions\n",
    "        pipe.fit(X_train, y_train)\n",
    "        predictions = pipe.predict(X_test)\n",
    "\n",
    "        # Calculate and store accuracy\n",
    "        accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "    # Calculate average accuracy for this iteration\n",
    "    avg_accuracy = np.mean(accuracy_scores)\n",
    "    overall_accuracy_scores.append(avg_accuracy)\n",
    "    print(f\"Iteration {iteration + 1}, Average Accuracy: {avg_accuracy}\")\n",
    "\n",
    "# Calculate and print the overall average accuracy across all iterations\n",
    "print(\"Overall Average Accuracy:\", np.mean(overall_accuracy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy dropping 'fed_party': 0.7608047138047139\n",
      "Average Accuracy dropping 'potus_party': 0.7447171717171717\n",
      "Average Accuracy dropping 'recess': 0.7534175084175084\n",
      "Average Accuracy dropping 'mom': 0.7231683501683503\n",
      "Average Accuracy dropping 'pce': 0.763888888888889\n",
      "Average Accuracy dropping 'ue': 0.7665521885521887\n",
      "Average Accuracy dropping 'cars': 0.7761010101010102\n",
      "Average Accuracy dropping 'house': 0.7695084175084175\n",
      "Average Accuracy dropping 'cli': 0.7819090909090909\n",
      "Average Accuracy dropping 'exports': 0.7817542087542088\n",
      "Average Accuracy dropping 'rgdp': 0.7745521885521887\n",
      "Average Accuracy dropping 'gdpd': 0.7637508417508418\n",
      "Average Accuracy dropping 'veloc': 0.7805353535353536\n",
      "Average Accuracy dropping 'ffr': 0.7824814814814814\n",
      "Average Accuracy dropping 'mich': 0.7782693602693603\n",
      "Average Accuracy dropping 'd_pce': 0.7841616161616162\n",
      "Average Accuracy dropping 'd_ue': 0.7866666666666667\n",
      "Average Accuracy dropping 'd_cars': 0.7835454545454545\n",
      "Average Accuracy dropping 'd_house': 0.7819191919191919\n",
      "Average Accuracy dropping 'd_cli': 0.7844848484848485\n",
      "Average Accuracy dropping 'd_exports': 0.7846498316498317\n",
      "Average Accuracy dropping 'd_rgdp': 0.7921582491582491\n",
      "Average Accuracy dropping 'd_gdpd': 0.7894309764309765\n",
      "Average Accuracy dropping 'd_veloc': 0.7703063973063973\n",
      "Average Accuracy dropping 'd_ffr': 0.782959595959596\n",
      "Average Accuracy dropping 'd_mich': 0.7868484848484849\n",
      "Average Accuracy dropping 'b0_spx': 0.7859360269360269\n",
      "Average Accuracy dropping 'b0_usd': 0.7846464646464646\n",
      "Average Accuracy dropping 'b0_loan': 0.7815892255892256\n",
      "Average Accuracy dropping 'b1_spx': 0.7965151515151516\n",
      "Average Accuracy dropping 'b1_usd': 0.7923703703703705\n",
      "Average Accuracy dropping 'b1_loan': 0.7835858585858586\n",
      "Average Accuracy dropping 'No Feature Dropped': 0.7945016835016834\n",
      "Feature Drop Accuracies: {'fed_party': 0.7608047138047139, 'potus_party': 0.7447171717171717, 'recess': 0.7534175084175084, 'mom': 0.7231683501683503, 'pce': 0.763888888888889, 'ue': 0.7665521885521887, 'cars': 0.7761010101010102, 'house': 0.7695084175084175, 'cli': 0.7819090909090909, 'exports': 0.7817542087542088, 'rgdp': 0.7745521885521887, 'gdpd': 0.7637508417508418, 'veloc': 0.7805353535353536, 'ffr': 0.7824814814814814, 'mich': 0.7782693602693603, 'd_pce': 0.7841616161616162, 'd_ue': 0.7866666666666667, 'd_cars': 0.7835454545454545, 'd_house': 0.7819191919191919, 'd_cli': 0.7844848484848485, 'd_exports': 0.7846498316498317, 'd_rgdp': 0.7921582491582491, 'd_gdpd': 0.7894309764309765, 'd_veloc': 0.7703063973063973, 'd_ffr': 0.782959595959596, 'd_mich': 0.7868484848484849, 'b0_spx': 0.7859360269360269, 'b0_usd': 0.7846464646464646, 'b0_loan': 0.7815892255892256, 'b1_spx': 0.7965151515151516, 'b1_usd': 0.7923703703703705, 'b1_loan': 0.7835858585858586, 'No Feature Dropped': 0.7945016835016834}\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data_cv('beta_dates/beta_data_2_42.csv', 'beta_dates/true_labels.csv')\n",
    "\n",
    "# Original features plus the case of dropping no feature\n",
    "original_features = X.columns.tolist()\n",
    "original_features.append(None)  # Represents the case of dropping no feature\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# Store average accuracies for each feature drop scenario\n",
    "feature_drop_accuracies = {}\n",
    "\n",
    "for drop_feature in original_features:\n",
    "    iteration_accuracies = []\n",
    "\n",
    "    for iteration in range(20):\n",
    "        # Drop one feature for this iteration, if specified\n",
    "        X_dropped = X.drop(columns=[drop_feature]) if drop_feature is not None else X\n",
    "\n",
    "        # Perform FAMD on the modified dataset\n",
    "        famd = FAMD(n_components=8)\n",
    "        famd.fit(X_dropped)\n",
    "        X_famd = famd.transform(X_dropped)\n",
    "\n",
    "        # Create a pipeline\n",
    "        pipe = Pipeline([\n",
    "            ('rf', RandomForestClassifier()),  # Assuming params are set for RandomForest\n",
    "        ])\n",
    "\n",
    "        pipe.set_params(**params)\n",
    "\n",
    "        accuracy_scores = []\n",
    "        for train_index, test_index in skf.split(X_famd, y):\n",
    "            X_train, X_test = X_famd.iloc[train_index], X_famd.iloc[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Fit the pipeline to the training data\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions on the test set\n",
    "            predictions = pipe.predict(X_test)\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "        # Calculate average accuracy for this iteration\n",
    "        iteration_accuracies.append(np.mean(accuracy_scores))\n",
    "\n",
    "    # Calculate the overall average accuracy after dropping the feature\n",
    "    avg_accuracy = np.mean(iteration_accuracies)\n",
    "    feature_name = drop_feature if drop_feature is not None else \"No Feature Dropped\"\n",
    "    feature_drop_accuracies[feature_name] = avg_accuracy\n",
    "    print(f\"Average Accuracy dropping '{feature_name}':\", avg_accuracy)\n",
    "\n",
    "# Print overall results\n",
    "print(\"Feature Drop Accuracies:\", feature_drop_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             \n",
      "Generation 1 - Current best internal CV score: 0.7898520084566596\n",
      "                                                                              \n",
      "Generation 2 - Current best internal CV score: 0.7942917547568711\n",
      "                                                                              \n",
      "Generation 3 - Current best internal CV score: 0.7942917547568711\n",
      "                                                                              \n",
      "Generation 4 - Current best internal CV score: 0.7942917547568711\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: 0.8081395348837208\n",
      "                                                                              \n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.55, min_samples_leaf=5, min_samples_split=6, n_estimators=100)\n",
      "Test score: 0.7454545454545455\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your data\n",
    "X, y = load_data_cv('beta_dates/beta_data_2_42.csv', 'beta_dates/true_labels.csv')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and run the TPOT classifier\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Export the best pipeline\n",
    "tpot.export('best_pipeline.py')\n",
    "\n",
    "# Evaluate the final model\n",
    "print(\"Test score:\", tpot.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9708029197080292\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load your data\n",
    "X, y = load_data_cv('beta_dates/beta_data_2_42.csv', 'beta_dates/true_labels.csv')\n",
    "\n",
    "# Specify K-Fold Cross-Validation\n",
    "n_splits = 5\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "# Instantiate and run the TPOT classifier with K-Fold CV\n",
    "tpot = TPOTClassifier(cv=cv, n_jobs=-1)\n",
    "tpot.fit(X, y)\n",
    "\n",
    "# Export the best pipeline\n",
    "tpot.export('new_best_pipeline.py')\n",
    "\n",
    "# Evaluate the final model using the test score function\n",
    "# Note: The test score function will perform a separate train-test split internally\n",
    "print(\"Test score:\", tpot.score(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.33      0.17      0.22         6\n",
      "         1.0       0.84      0.91      0.88        47\n",
      "         2.0       0.87      0.81      0.84        16\n",
      "\n",
      "    accuracy                           0.83        69\n",
      "   macro avg       0.68      0.63      0.65        69\n",
      "weighted avg       0.80      0.83      0.81        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "# load the data without the index column\n",
    "tpot_data = pd.read_csv('famd_data.csv', index_col=0)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=None)\n",
    "\n",
    "# Average CV score on the training set was: 0.8322558922558922\n",
    "exported_pipeline = make_pipeline(\n",
    "    SelectPercentile(score_func=f_classif, percentile=39),\n",
    "    GradientBoostingClassifier(learning_rate=0.1, max_depth=5, max_features=0.9000000000000001, min_samples_leaf=13, min_samples_split=9, n_estimators=100, subsample=0.9500000000000001)\n",
    ")\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "print(classification_report(testing_target, results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
