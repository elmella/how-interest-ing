{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from prince import FAMD\n",
    "from scipy import sparse\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df, col_split):\n",
    "    \"\"\"\n",
    "    Prepares the data for the model by standardizing the continuous features, \n",
    "    converting the categorical features to strings, \n",
    "    and converting sparse columns to a dense format for FAMD.\n",
    "    \"\"\"\n",
    "    categorical_cols = df.columns[:col_split]\n",
    "\n",
    "    # converting the categorical features to strings\n",
    "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(data_address='beta_dates/beta_data_2_42.csv', label_address='beta_dates/true_labels.csv'):\n",
    "    df = pd.read_csv(data_address, index_col=0)\n",
    "    y = pd.read_csv(label_address, index_col=0).values[:,0]\n",
    "    y = y + 1\n",
    "    X = encoder(df, 4)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7818181818181819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.30      0.43        10\n",
      "         1.0       0.77      0.94      0.85        35\n",
      "         2.0       0.88      0.70      0.78        10\n",
      "\n",
      "    accuracy                           0.78        55\n",
      "   macro avg       0.80      0.65      0.68        55\n",
      "weighted avg       0.78      0.78      0.76        55\n",
      "\n",
      "Test Accuracy: 0.8363636363636363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.40      0.50         5\n",
      "         1.0       0.91      0.89      0.90        44\n",
      "         2.0       0.56      0.83      0.67         6\n",
      "\n",
      "    accuracy                           0.84        55\n",
      "   macro avg       0.71      0.71      0.69        55\n",
      "weighted avg       0.85      0.84      0.84        55\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest_famd_model.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Parameters for Random Forest\n",
    "params = {\n",
    "    'rf__max_depth': 4,\n",
    "    'rf__max_features': 0.65,\n",
    "    'rf__min_samples_leaf': 6,\n",
    "    'rf__min_samples_split': 17,\n",
    "    'rf__n_estimators': 200\n",
    "}\n",
    "\n",
    "# Create the pipeline with FAMD and RandomForestClassifier\n",
    "pipe = Pipeline([\n",
    "    ('famd', FAMD(n_components=6)),  # FAMD with 6 components\n",
    "    ('rf', RandomForestClassifier()) # Random Forest Classifier\n",
    "])\n",
    "\n",
    "# Set the parameters for the pipeline\n",
    "pipe.set_params(**params)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_accuracy = pipe.score(X_test, y_test)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "# Print the classification report on the test data\n",
    "print(classification_report(y_test, pipe.predict(X_test)))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipe, 'random_forest_famd_model.pkl')\n",
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Apply FAMD for dimensionality reduction\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "# Create a pipeline with the best parameters\n",
    "pipe = Pipeline([\n",
    "    ('xgb', XGBClassifier(objective='multi:softmax'))  # Example for a classification task\n",
    "])\n",
    "\n",
    "# Set the parameters for the pipeline\n",
    "params = {\n",
    "    'xgb__colsample_bytree': 0.8314087089720534, \n",
    "    'xgb__gamma': 2.068218435894297, \n",
    "    'xgb__learning_rate': 0.7757676744286774, \n",
    "    'xgb__max_depth': 4, \n",
    "    'xgb__min_child_weight': 5.920879982985454, \n",
    "    'xgb__n_estimators': 156, \n",
    "    'xgb__reg_alpha': 2.0124120042376505, \n",
    "    'xgb__reg_lambda': 4.703662676675085, \n",
    "    'xgb__subsample': 0.7918852620425526\n",
    "}\n",
    "pipe.set_params(**params)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = pipe.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipe, 'random_forest_famd_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores: [0.8363636363636363, 0.7454545454545455, 0.7818181818181819, 0.8181818181818182, 0.8333333333333334] \n",
      "\n",
      "Average Accuracy: 0.8030303030303031\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('rf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# Set pipeline parameters\n",
    "params = {'rf__max_depth': 4, 'rf__max_features': 0.65, 'rf__min_samples_leaf': 6, 'rf__min_samples_split': 17, 'rf__n_estimators': 200}\n",
    "pipe.set_params(**params)\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# List to store accuracy scores for this iteration\n",
    "accuracy_scores = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, test_index in skf.split(X_transformed, y):\n",
    "    X_train, X_test = X_transformed.iloc[train_index], X_transformed.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the pipeline and make predictions\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions = pipe.predict(X_test)\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Calculate average accuracy for this iteration\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print('Accuracy Scores: {} \\n'.format(accuracy_scores))\n",
    "print('Average Accuracy: {}'.format(avg_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores: [0.7454545454545455, 0.7454545454545455, 0.8363636363636363, 0.7454545454545455, 0.7777777777777778] \n",
      "\n",
      "Average Accuracy: 0.7701010101010101\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X = famd.transform(X)\n",
    "\n",
    "\n",
    "# create a pipeline with the best parameters\n",
    "pipe = Pipeline([\n",
    "    ('xgb', XGBClassifier(objective='multi:softmax'))  # Example for a classification task\n",
    "])\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "\n",
    "# set the parameters to the mean values\n",
    "params = {\n",
    "    'xgb__colsample_bytree': 0.8314087089720534, \n",
    "    'xgb__gamma': 2.068218435894297, \n",
    "    'xgb__learning_rate': 0.7757676744286774, \n",
    "    'xgb__max_depth': 4, \n",
    "    'xgb__min_child_weight': 5.920879982985454, \n",
    "    'xgb__n_estimators': 156, \n",
    "    'xgb__reg_alpha': 2.0124120042376505, \n",
    "    'xgb__reg_lambda': 4.703662676675085, \n",
    "    'xgb__subsample': 0.7918852620425526}\n",
    "\n",
    "pipe.set_params(**params)\n",
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# List to store accuracy scores for this iteration\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the pipeline to the training data\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipe.predict(X_test)\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Calculate average accuracy for this iteration\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print('Accuracy Scores: {} \\n'.format(accuracy_scores))\n",
    "print('Average Accuracy: {}'.format(avg_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RF Accuracy: 0.516969696969697\n",
      "Average XGB Accuracy: 0.5454545454545453\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Parameters for both Random Forest and XGBClassifier\n",
    "rf_params = {\n",
    "    'rf__max_depth': 4,\n",
    "    'rf__max_features': 0.65,\n",
    "    'rf__min_samples_leaf': 6,\n",
    "    'rf__min_samples_split': 17,\n",
    "    'rf__n_estimators': 200\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'xgb__colsample_bytree': 0.8314087089720534, \n",
    "    'xgb__gamma': 2.068218435894297, \n",
    "    'xgb__learning_rate': 0.7757676744286774, \n",
    "    'xgb__max_depth': 4, \n",
    "    'xgb__min_child_weight': 5.920879982985454, \n",
    "    'xgb__n_estimators': 156, \n",
    "    'xgb__reg_alpha': 2.0124120042376505, \n",
    "    'xgb__reg_lambda': 4.703662676675085, \n",
    "    'xgb__subsample': 0.7918852620425526\n",
    "}\n",
    "\n",
    "# Number of iterations\n",
    "iterations = 30\n",
    "\n",
    "# Initialize lists to store accuracies for each model\n",
    "rf_accuracies = []\n",
    "xgb_accuracies = []\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Split the data for Random Forest pipeline\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    split_index = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Create and fit the Random Forest pipeline\n",
    "    rf_pipe = Pipeline([\n",
    "        ('famd', FAMD(n_components=6)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ])\n",
    "    rf_pipe.set_params(**rf_params)\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Random Forest model\n",
    "    rf_accuracies.append(rf_pipe.score(X_test, y_test))\n",
    "\n",
    "    # Apply FAMD and create the XGBClassifier pipeline\n",
    "    famd = FAMD(n_components=6)\n",
    "    famd.fit(X_train)\n",
    "    X_xgb_train_transformed = famd.transform(X_train)\n",
    "    X_xgb_test_transformed = famd.transform(X_test)\n",
    "\n",
    "    xgb_pipe = Pipeline([\n",
    "        ('xgb', XGBClassifier(objective='multi:softmax'))\n",
    "    ])\n",
    "    xgb_pipe.set_params(**xgb_params)\n",
    "    xgb_pipe.fit(X_xgb_train_transformed, y_train)\n",
    "\n",
    "    # Evaluate the XGBClassifier model\n",
    "    xgb_accuracies.append(xgb_pipe.score(X_xgb_test_transformed, y_test))\n",
    "\n",
    "# Calculate the average accuracy for each model\n",
    "avg_rf_accuracy = np.mean(rf_accuracies)\n",
    "avg_xgb_accuracy = np.mean(xgb_accuracies)\n",
    "\n",
    "# Print the average accuracies\n",
    "print('Average RF Accuracy:', avg_rf_accuracy)\n",
    "print('Average XGB Accuracy:', avg_xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 5, 1.0: 30, 2.0: 20}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find how many counts of each class are in the y_test\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "30/55"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
