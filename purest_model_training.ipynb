{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from prince import FAMD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df, col_split):\n",
    "    \"\"\"\n",
    "    Prepares the data for the model by standardizing the continuous features, \n",
    "    converting the categorical features to strings, \n",
    "    and converting sparse columns to a dense format for FAMD.\n",
    "    \"\"\"\n",
    "    categorical_cols = df.columns[:col_split]\n",
    "    # converting the categorical features to strings\n",
    "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
    "    return df\n",
    "\n",
    "def load_data(data_address='beta_dates/beta_data_2_42.csv', label_address='beta_dates/true_labels.csv'):\n",
    "    df = pd.read_csv(data_address, index_col=0)\n",
    "    y = pd.read_csv(label_address, index_col=0).values[:,0]\n",
    "    y = y + 1\n",
    "    X = encoder(df, 4)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7636363636363637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.40      0.50         5\n",
      "         1.0       0.79      0.95      0.86        39\n",
      "         2.0       0.60      0.27      0.37        11\n",
      "\n",
      "    accuracy                           0.76        55\n",
      "   macro avg       0.68      0.54      0.58        55\n",
      "weighted avg       0.74      0.76      0.73        55\n",
      "\n",
      "Test Accuracy: 0.7636363636363637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.40      0.50         5\n",
      "         1.0       0.83      0.85      0.84        40\n",
      "         2.0       0.55      0.60      0.57        10\n",
      "\n",
      "    accuracy                           0.76        55\n",
      "   macro avg       0.68      0.62      0.64        55\n",
      "weighted avg       0.76      0.76      0.76        55\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['random_forest_famd_model.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Parameters for Random Forest\n",
    "params = {\n",
    "    'rf__max_depth': 4,\n",
    "    'rf__max_features': 0.65,\n",
    "    'rf__min_samples_leaf': 6,\n",
    "    'rf__min_samples_split': 17,\n",
    "    'rf__n_estimators': 200\n",
    "}\n",
    "\n",
    "# Create the pipeline with FAMD and RandomForestClassifier\n",
    "pipe = Pipeline([\n",
    "    ('famd', FAMD(n_components=6)),  # FAMD with 6 components\n",
    "    ('rf', RandomForestClassifier()) # Random Forest Classifier\n",
    "])\n",
    "\n",
    "# Set the parameters for the pipeline\n",
    "pipe.set_params(**params)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_accuracy = pipe.score(X_test, y_test)\n",
    "print('Test Accuracy:', test_accuracy)\n",
    "\n",
    "# Print the classification report on the test data\n",
    "print(classification_report(y_test, pipe.predict(X_test)))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipe, 'random_forest_famd_model.pkl')\n",
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Apply FAMD for dimensionality reduction\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "# Create a pipeline with the best parameters\n",
    "pipe = Pipeline([\n",
    "    ('xgb', XGBClassifier(objective='multi:softmax'))  # Example for a classification task\n",
    "])\n",
    "\n",
    "# Set the parameters for the pipeline\n",
    "params = {\n",
    "    'xgb__colsample_bytree': 0.8314087089720534, \n",
    "    'xgb__gamma': 2.068218435894297, \n",
    "    'xgb__learning_rate': 0.7757676744286774, \n",
    "    'xgb__max_depth': 4, \n",
    "    'xgb__min_child_weight': 5.920879982985454, \n",
    "    'xgb__n_estimators': 156, \n",
    "    'xgb__reg_alpha': 2.0124120042376505, \n",
    "    'xgb__reg_lambda': 4.703662676675085, \n",
    "    'xgb__subsample': 0.7918852620425526\n",
    "}\n",
    "pipe.set_params(**params)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = pipe.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print('Test Accuracy:', accuracy)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(pipe, 'random_forest_famd_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores: [0.8, 0.8181818181818182, 0.8181818181818182, 0.8, 0.7962962962962963] \n",
      "\n",
      "Average Accuracy: 0.8065319865319864\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('rf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# Set pipeline parameters\n",
    "params = {'rf__max_depth': 4, 'rf__max_features': 0.65, 'rf__min_samples_leaf': 6, 'rf__min_samples_split': 17, 'rf__n_estimators': 200}\n",
    "pipe.set_params(**params)\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# List to store accuracy scores for this iteration\n",
    "accuracy_scores = []\n",
    "model_list = []\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "for train_index, test_index in skf.split(X_transformed, y):\n",
    "    X_train, X_test = X_transformed.iloc[train_index], X_transformed.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the pipeline and make predictions\n",
    "    pipe.fit(X_train, y_train)\n",
    "    predictions = pipe.predict(X_test)\n",
    "\n",
    "    # Calculate and store accuracy\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "    # Store the model\n",
    "    model_list.append(pipe)\n",
    "\n",
    "# Calculate average accuracy for this iteration\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print('Accuracy Scores: {} \\n'.format(accuracy_scores))\n",
    "print('Average Accuracy: {}'.format(avg_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7262773722627737\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "X, y = load_data()\n",
    "y_guesses = np.ones(len(y))\n",
    "for i in range(1, len(y)):\n",
    "    y_guesses[i] = y[i-1]\n",
    "\n",
    "print(accuracy_score(y, y_guesses))\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "# print(pipe.predict(X_transformed))\n",
    "# print(y_guesses)\n",
    "for pipe in model_list:\n",
    "    diff = y_guesses != pipe.predict(X_transformed)\n",
    "    # print(pipe.predict(X_transformed)[diff])\n",
    "    # print(y[diff])\n",
    "    # print(y_guesses[diff])\n",
    "\n",
    "    print(accuracy_score(pipe.predict(X_transformed)[diff], y[diff]))\n",
    "    # print(accuracy_score(y_guesses[diff], y[diff]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Scores: [0.7090909090909091, 0.7272727272727273, 0.8, 0.8181818181818182, 0.8148148148148148] \n",
      "\n",
      "Average Accuracy: 0.7738720538720539\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X = famd.transform(X)\n",
    "\n",
    "\n",
    "# create a pipeline with the best parameters\n",
    "pipe = Pipeline([\n",
    "    ('xgb', XGBClassifier(objective='multi:softmax'))  # Example for a classification task\n",
    "])\n",
    "\n",
    "# Perform K-Fold Cross-Validation\n",
    "\n",
    "# set the parameters to the mean values\n",
    "params = {\n",
    "    'xgb__colsample_bytree': 0.8314087089720534, \n",
    "    'xgb__gamma': 2.068218435894297, \n",
    "    'xgb__learning_rate': 0.7757676744286774, \n",
    "    'xgb__max_depth': 4, \n",
    "    'xgb__min_child_weight': 5.920879982985454, \n",
    "    'xgb__n_estimators': 156, \n",
    "    'xgb__reg_alpha': 2.0124120042376505, \n",
    "    'xgb__reg_lambda': 4.703662676675085, \n",
    "    'xgb__subsample': 0.7918852620425526}\n",
    "\n",
    "pipe.set_params(**params)\n",
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# List to store accuracy scores for this iteration\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Fit the pipeline to the training data\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    predictions = pipe.predict(X_test)\n",
    "\n",
    "    accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Calculate average accuracy for this iteration\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "\n",
    "# Print the accuracy scores\n",
    "print('Accuracy Scores: {} \\n'.format(accuracy_scores))\n",
    "print('Average Accuracy: {}'.format(avg_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RF Accuracy: 0.5109090909090909\n",
      "Average XGB Accuracy: 0.5454545454545453\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Parameters for both Random Forest and XGBClassifier\n",
    "rf_params = {\n",
    "    'rf__max_depth': 4,\n",
    "    'rf__max_features': 0.65,\n",
    "    'rf__min_samples_leaf': 6,\n",
    "    'rf__min_samples_split': 17,\n",
    "    'rf__n_estimators': 200\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'xgb__colsample_bytree': 0.8314087089720534, \n",
    "    'xgb__gamma': 2.068218435894297, \n",
    "    'xgb__learning_rate': 0.7757676744286774, \n",
    "    'xgb__max_depth': 4, \n",
    "    'xgb__min_child_weight': 5.920879982985454, \n",
    "    'xgb__n_estimators': 156, \n",
    "    'xgb__reg_alpha': 2.0124120042376505, \n",
    "    'xgb__reg_lambda': 4.703662676675085, \n",
    "    'xgb__subsample': 0.7918852620425526\n",
    "}\n",
    "\n",
    "# Number of iterations\n",
    "iterations = 30\n",
    "\n",
    "# Initialize lists to store accuracies for each model\n",
    "rf_accuracies = []\n",
    "xgb_accuracies = []\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Split the data for Random Forest pipeline\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    split_index = int(len(X) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Create and fit the Random Forest pipeline\n",
    "    rf_pipe = Pipeline([\n",
    "        ('famd', FAMD(n_components=6)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ])\n",
    "    rf_pipe.set_params(**rf_params)\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the Random Forest model\n",
    "    rf_accuracies.append(rf_pipe.score(X_test, y_test))\n",
    "\n",
    "    # Apply FAMD and create the XGBClassifier pipeline\n",
    "    famd = FAMD(n_components=6)\n",
    "    famd.fit(X_train)\n",
    "    X_xgb_train_transformed = famd.transform(X_train)\n",
    "    X_xgb_test_transformed = famd.transform(X_test)\n",
    "\n",
    "    xgb_pipe = Pipeline([\n",
    "        ('xgb', XGBClassifier(objective='multi:softmax'))\n",
    "    ])\n",
    "    xgb_pipe.set_params(**xgb_params)\n",
    "    xgb_pipe.fit(X_xgb_train_transformed, y_train)\n",
    "\n",
    "    # Evaluate the XGBClassifier model\n",
    "    xgb_accuracies.append(xgb_pipe.score(X_xgb_test_transformed, y_test))\n",
    "\n",
    "# Calculate the average accuracy for each model\n",
    "avg_rf_accuracy = np.mean(rf_accuracies)\n",
    "avg_xgb_accuracy = np.mean(xgb_accuracies)\n",
    "\n",
    "# Print the average accuracies\n",
    "print('Average RF Accuracy:', avg_rf_accuracy)\n",
    "print('Average XGB Accuracy:', avg_xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Parameters for Random Forest\n",
    "rf_params = {\n",
    "    'rf__max_depth': 4,\n",
    "    'rf__max_features': 0.65,\n",
    "    'rf__min_samples_leaf': 6,\n",
    "    'rf__min_samples_split': 17,\n",
    "    'rf__n_estimators': 200\n",
    "}\n",
    "\n",
    "# Parameters for XGBClassifier\n",
    "xgb_params = {\n",
    "    'xgb__colsample_bytree': 1.0, \n",
    "    'xgb__gamma': 3.646092407548312, \n",
    "    'xgb__learning_rate': 0.01, \n",
    "    'xgb__max_depth': 6, \n",
    "    'xgb__min_child_weight': 9.488817186497869, \n",
    "    'xgb__n_estimators': 81, \n",
    "    'xgb__reg_alpha': 1.5988455864986395, \n",
    "    'xgb__reg_lambda': 3.267653422896763, \n",
    "    'xgb__subsample': 1.0}\n",
    "\n",
    "# split the rf_params and xgb_params on __, and get the split and final index\n",
    "rf_param_names = {param.split('__')[1]:rf_params[param] for param in rf_params.keys()}\n",
    "xgb_param_names = {param.split('__')[1]:xgb_params[param] for param in xgb_params.keys()}\n",
    "split_index, final_index = int(len(X) * 0.5), len(X)\n",
    "\n",
    "# Initialize lists to store accuracies for each model\n",
    "rf_accuracies = []\n",
    "xgb_accuracies = []\n",
    "\n",
    "# Repeat the experiment 20 times and initialize the prediction lists\n",
    "iterations = 4\n",
    "for j in range(iterations):\n",
    "    rf_predictions = []\n",
    "    xgb_predictions = []\n",
    "    y_actual = []\n",
    "    \n",
    "    # Iterate through the data and get the train test split based on the split day\n",
    "    for i in range(split_index, final_index):\n",
    "        # Get the FAMD transformation\n",
    "        X_current = X.iloc[:i+1]\n",
    "        famd = FAMD(n_components=6, engine='sklearn')\n",
    "        famd.fit(X_current)\n",
    "        X_famd = famd.transform(X_current)\n",
    "\n",
    "        # Get the train test split\n",
    "        X_train, X_test = X_famd[:i], X_famd[i:i+1]\n",
    "        y_train, y_test = y[:i], y[i:i+1]\n",
    "\n",
    "        # Create and fit the Random Forest classifier, append the prediction to rf_guess\n",
    "        rf_classifier = RandomForestClassifier(warm_start=False)\n",
    "        rf_classifier.set_params(**rf_param_names)\n",
    "        rf_classifier.fit(X_train, y_train)\n",
    "        rf_predictions.append(rf_classifier.predict(X_test))\n",
    "\n",
    "        # Create and fit the XGBClassifier, append the prediction to xgb_guess\n",
    "        xgb_classifier = XGBClassifier()\n",
    "        xgb_classifier.set_params(**xgb_param_names)\n",
    "        xgb_classifier.fit(X_train, y_train)\n",
    "        xgb_predictions.append(xgb_classifier.predict(X_test))\n",
    "\n",
    "        # Append the actual value to y_actual\n",
    "        y_actual.append(y_test)\n",
    "\n",
    "    # Calculate the accuracy scores for each model\n",
    "    rf_accuracy = accuracy_score(np.array(y_actual), np.array(rf_predictions))\n",
    "    xgb_accuracy = accuracy_score(np.array(y_actual), np.array(xgb_predictions))\n",
    "    rf_accuracies.append(rf_accuracy)\n",
    "    xgb_accuracies.append(xgb_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RF Accuracy: 0.8094890510948906\n",
      "Average XGB Accuracy: 0.7883211678832116\n"
     ]
    }
   ],
   "source": [
    "# print the mean accuracy scores for each model\n",
    "print('Average RF Accuracy:', np.mean(rf_accuracies))\n",
    "print('Average XGB Accuracy:', np.mean(xgb_accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
