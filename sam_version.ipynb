{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('master_data.csv', index_col=0)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.loc['1989-08-23':] #df = df.loc['1989-07-7':]\n",
    "df['mom'] = -df['mom_d'] + df['mom_u']\n",
    "df = df.drop(columns=['mom_u', 'mom_d'])\n",
    "df['label_day'] = ~df['decision'].isna()\n",
    "df = df.fillna(method='ffill')\n",
    "df = df.iloc[1:]\n",
    "df.to_csv('master_data_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do...\n",
    "fix the decision column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the columns\n",
    "monthly = ['pce','ue','cars','house','cli','exports','rgdp','gdpd','veloc', 'ffr','mich']\n",
    "labels = ['decision', 'ffr', 'change']\n",
    "daily = ['spx','usd', 'loan']\n",
    "categorical = ['fed_party','potus_party','recess', 'mom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'label_day'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# union of all columns\n",
    "all_cols = monthly + daily + categorical + labels\n",
    "shared = set(df.columns.intersection(all_cols).to_list())\n",
    "df_cols = df.columns.to_list()\n",
    "\n",
    "# find the unique values of all_cols\n",
    "unique_ourlabels = {col if col not in shared else None for col in all_cols}\n",
    "unique_dflabels = {col if col not in shared else None for col in df_cols}\n",
    "unique_ourlabels.remove(None)\n",
    "unique_dflabels.remove(None)\n",
    "display(unique_ourlabels)\n",
    "display(unique_dflabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward fill all nan values\n",
    "def collapse(df, key = 'label_day', start = 7, end = 60, cat_col = [], daily_col = [], other = [], labels = []):\n",
    "    # Get our different dataframes\n",
    "    categorical = df[cat_col]\n",
    "    daily = df[daily_col]\n",
    "    monthly = df[other]\n",
    "    keep = df[key]\n",
    "    labels = df[labels]\n",
    "\n",
    "    # create the windows\n",
    "    event_days = df[df[key]].index\n",
    "\n",
    "    ################## Handle Categorical and Label Data ##################\n",
    "    cat_collapsed = categorical[keep]\n",
    "    y_labels = labels[keep]\n",
    "\n",
    "    ################## Handle Monthly Data ##################\n",
    "    # Create event windows\n",
    "    windows = []\n",
    "    for day in event_days:\n",
    "        # Calculate start and end days of the window\n",
    "        start_day = day - pd.Timedelta(days=start)\n",
    "        end_day = day - pd.Timedelta(days=end)\n",
    "\n",
    "        # Ensure start_day is not before the start of the dataset\n",
    "        if end_day < df.index[0]:\n",
    "            end_day = df.index[0]\n",
    "\n",
    "        windows.append((start_day, end_day))\n",
    "\n",
    "    # Initialize a list to store aggregated data\n",
    "    aggregated_data = []\n",
    "\n",
    "    for (start_day, end_day) in windows:\n",
    "        # Select data within the window\n",
    "        window_data = monthly.loc[end_day:start_day]\n",
    "\n",
    "        # Apply your custom aggregation function\n",
    "        final_vals = window_data.loc[start_day]\n",
    "        change = final_vals - window_data.iloc[0]\n",
    "        # relabel the change columns to include a d_ prefix\n",
    "        change = change.rename(lambda x: 'd_' + x)\n",
    "\n",
    "        # concatenate the data horizontally\n",
    "        aggregated_window = pd.concat([final_vals, change], axis=0)\n",
    "        aggregated_data.append(aggregated_window)\n",
    "\n",
    "    # Combine aggregated data\n",
    "    aggregated_monthly = pd.concat(aggregated_data, axis=1).T\n",
    "    aggregated_monthly.index = event_days\n",
    "\n",
    "    ################## Handle Daily Data ##################\n",
    "    # define the model\n",
    "    model = linear_model.LinearRegression()\n",
    "    daily_agg = []\n",
    "\n",
    "    # Get the names of the features\n",
    "    daily_feat = daily.columns\n",
    "    b0_name = ['b0_' + feat for feat in daily_feat]\n",
    "    b1_name = ['b1_' + feat for feat in daily_feat]\n",
    "\n",
    "    for (start_day, end_day) in windows:\n",
    "        # Select data within the window\n",
    "        window_data = daily.loc[end_day:start_day]\n",
    "        time = np.arange(len(window_data)).reshape(-1, 1)\n",
    "\n",
    "        # Initialize lists to store betas for each feature\n",
    "        beta0 = []\n",
    "        beta1 = []\n",
    "\n",
    "        # Loop through each feature and fit the model\n",
    "        for feat in daily_feat:\n",
    "            model.fit(time, window_data[feat])\n",
    "            beta0.append(model.intercept_)\n",
    "            beta1.append(model.coef_[0])\n",
    "\n",
    "        # Create DataFrames for betas\n",
    "        beta0_df = pd.DataFrame([beta0], columns=b0_name)\n",
    "        beta1_df = pd.DataFrame([beta1], columns=b1_name)\n",
    "\n",
    "        # concatenate the data horizontally\n",
    "        aggregated_window = pd.concat([beta0_df, beta1_df], axis=1)\n",
    "        daily_agg.append(aggregated_window)\n",
    "\n",
    "    # Combine aggregated data\n",
    "    aggregated_daily = pd.concat(daily_agg, axis=0)\n",
    "    aggregated_daily.index = event_days\n",
    "\n",
    "    ################## Combine Data ##################\n",
    "    # Combine all data\n",
    "    combined = pd.concat([cat_collapsed, aggregated_monthly, aggregated_daily], axis=1)\n",
    "    return combined, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ranges = [(7,60),(2,60),(7,120),(2,120),(7,42),(2,42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the date ranges and save the data\n",
    "for dates in date_ranges:\n",
    "    beta_data, label_data = collapse(df, start = dates[0], end = dates[1], cat_col = categorical, daily_col = daily, other = monthly, labels = labels)\n",
    "    beta_data.to_csv('beta_dates/beta_data_' + str(dates[0]) + '_' + str(dates[1]) + '.csv')\n",
    "label_data.to_csv('beta_dates/true_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50 (+/- 0.32)\n",
      "Accuracy: 0.48 (+/- 0.26)\n",
      "Accuracy: 0.50 (+/- 0.30)\n",
      "Accuracy: 0.49 (+/- 0.30)\n",
      "Accuracy: 0.49 (+/- 0.29)\n",
      "Accuracy: 0.49 (+/- 0.27)\n"
     ]
    }
   ],
   "source": [
    "# Upload the right data\n",
    "names = [\"7_60\", \"2_60\", \"7_120\", \"2_120\", \"7_42\", \"2_42\"]\n",
    "for name in names:\n",
    "    data = pd.read_csv('beta_dates/beta_data_'+name+'.csv', index_col=0)\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "\n",
    "    # Load in the labels\n",
    "    labels = pd.read_csv('beta_dates/true_labels.csv', index_col=0)\n",
    "    labels.index = pd.to_datetime(labels.index)\n",
    "\n",
    "    # fit a sklearn softmax regression\n",
    "    def cross_val(data, labels, folds = 6):\n",
    "        # define the model with an intercept and l2 regularization\n",
    "        model = linear_model.LogisticRegression(penalty='l2', fit_intercept=True, multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "        # fit the model\n",
    "        model.fit(data, labels)\n",
    "\n",
    "        # get the cross validation scores\n",
    "        scores = cross_val_score(model, data, labels, cv=folds)\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "        # get the coefficients\n",
    "        coef = model.coef_\n",
    "        return coef\n",
    "\n",
    "\n",
    "    #normalize the data\n",
    "    def normalize(data):\n",
    "        return (data - data.mean()) / data.std()\n",
    "\n",
    "    normalized_data = data.values\n",
    "    normalized_data = normalize(data).values\n",
    "    vals = cross_val(normalized_data, labels['decision'])\n",
    "    coef_size = np.linalg.norm(vals, axis=0)\n",
    "\n",
    "    # what are the smallest coefficients of coef_size\n",
    "    coef_size = pd.DataFrame(coef_size, index=data.columns)\n",
    "    coef_size.columns = ['coef_size']\n",
    "    coef_size = coef_size.sort_values(by='coef_size', ascending=False)\n",
    "\n",
    "    # save coef_size as a csv\n",
    "    coef_size.to_csv('feature_importance/coef_size'+name+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
