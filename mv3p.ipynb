{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Federal Funds Rate Changes\n",
    "    Xander de la Bruere\n",
    "    Jakob Gertsch\n",
    "    Sam Layton\n",
    "    Matt Mella\n",
    "    Wilson Stoddard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from datetime import date, timedelta\n",
    "import fredapi as fd\n",
    "from prince import FAMD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from prince import FAMD\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The Federal Reserve (Fed) is a United States governmental institution that serves to keep the national economy stable and under control. By far the most powerful and influntial economic organization in the nation, the Fed is tasked with promoting \"maximum employment, stable prices, and moderate long-term interest rates\" [1]. Significantly, while this purpose statement contains three distinct items, the Fed's mission is commonly referred to as the \"dual mandate\". This is because moderating long-term interest rates actually forms the mechanism by which employment is maximized and inflation is curved.\n",
    "\n",
    "In particular, since long-term interest rates essentially represent the cost of borowing money, a change in the prevailing interest rate significantly impacts levels of personal expenditures as well as corporate investment.  For example, when interest rates are relatively high, consumers have greater incentive to save money (and earn interest) rather than spend money, while companies have less incentive to invest (since they must pay a higher interest rate on their loans). This leads to an overall cooling in the economy. Conversely, lower interest rates increase personal consumption and corporate investment, causing the economy to heat up.\n",
    "\n",
    "Long-term interest rates also have important implications for personal and institutional investment. For example, since bond prices are known to have an inverse relationship with bond prices, portfolios that are heavy in bonds lose value quickly when interest rates spike, leading to a painful loss in returns for investors [2]. In addition, even equity portfolios are negatively impacted by higher interest rates; since companies will spend less on new projects when the cost of borrowing is high, corporate profits taper, resulting in lower stock prices.\n",
    "\n",
    "In the United States, the most important long-term interest rate is the federal funds rate (FFR), a target interest rate set by the Fed. When the Fed sets a new FFR, the Fed moves the prevailing interest rate observed in the economy towards this target interest rate through means such as requiring banks to keep more or less money in reserve or adjusting the number of federal bonds available on the market (thus increasing or decreasing the supply of money available for borrowing) [3]. As a result, the FFR almost always becomes the prevailing interest rate in the economy. Thus the FFR, through its effect on each of the personal and corporate spending and investment decisions discussed previously, has dramatic effects on a myriad of important economic indicators, including inflation and unemployment.\n",
    "\n",
    "### Research Objectives\n",
    "\n",
    "Due to the power that the FFR wields in the U.S. economy, the ability to predict changes in the FFR would allow consumers, corporations, and investment managers to make optimal financial decisions. For this reason, much effort has been expended to predict changes in the FFR. The Fed has shared some indicators that it considers in its decisions, including various price indices and labor market indicators, such as the annual change in the price index for personal consumption expenditures [1]. However, it does not explicitly share the way it considers or weights these metrics. Other groups that have attempted prediction, such as the Blue Chip consensus, are relatively successful at predicting changes in the FFR when most or all economic indicators are optimal (or adverse), but are inconsistent in accuracy when economic conditions are mixed [4]. We seek to build upon this work.\n",
    "\n",
    "Our objective here is to use the techniques of regression and classification to develop a more robust model for predicting changes in the FFR. We are also concerned with finding features that best predict FOMC decisions to raise, lower, or hold constant the FFR. Since the naive prediction method of always predicting no change (the most common outcome) yields an accuracy score of 68% historically, we seek to develop models and methods that outperform this baseline in particular.\n",
    "\n",
    "In short, we seek to answer these three key questions: 1) What features have the most predictive power for forecasting FOMC decisions? 2) What regression and classification methods are best suited to forecasting these decisions? 3) How accurately can we predict FOMC decisions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Data\n",
    "\n",
    "In our search for data on key economic indicators that might influence the FFR, we looked to the St. Louis Federal Reserve (FRED) website [5]. This site hosts an enormous quantity of time series data on economic indicators. This data has been carefully and methodically tracked, and is a gold standard for economic data. Many reputable news sources, including the Wall Street Journal, frequently cite FRED data. In selecting features, we focused on features with data continuously tracked since 1989 (the year our FOMC decisions data starts) to the present day. We also chose features that we felt were most likely to have predictive power in determining the FOMC's decision to raise or lower interest rate. With these two considerations in mind, we selected the several features which are summarized in the table below.\n",
    "\n",
    "| Feature | Variable | Frequency | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| Bank Prime Loan Rate Changes | `loan` | daily | Rate charged by banks for short-term loans to creditworthy debtors |\n",
    "| Exports of Goods and Services | `export` | quarterly | Total dollar value of goods and services exported in the quarter |\n",
    "| Personal Consumption Expenditures Rate | `pce` | monthly | Measure of core inflation for personal expenditures |\n",
    "| Unemployment Rate | `ue` | monthly | Number of unemployed as a percentage of the labor force |\n",
    "| Change in Real GDP | `rgdp` | quarterly | Quarterly change in inflation-adjusted GDP |\n",
    "| Total Vehicle Sales | `cars` | monthly | Total number of vehicle sales in millions |\n",
    "| Recession Indicator | `recess` | monthly | Binary variable indicating whether the US is in a recession |\n",
    "| GDP Deflator | `gdpd` | daily | Price index given by ratio of nominal GDP to real GDP |\n",
    "| Velocity of M1 Money | `veloc` | quarterly | Ratio of nominal GDP to the quarterly average of M1 money stock |\n",
    "| New Private Housing Units Started | `house` | monthly | Number of new housing units beginning construction in millions  |\n",
    "\n",
    "In addition, we hypothesized that stock market performance and the strength of the dollar might be important factors in the FOMC's decision. The stock market acts as a barometer for the health of the economy, and the strength of the dollar indicates the strength of the economy relative to other countries. Thus, we used daily closing price data from Yahoo! Finance for the S&P 500 (`spx`, a proxy for total stock market) and US Dollar (`usd`, the value of the USD versus a basket of foreign currencies) Indices [6].\n",
    "\n",
    "Considering the differences in opinion on economic policy that undoubtedly factor into the FOMC's decisions to raise or lower interest rates, we also generated binary variables for the political party of the Fed Chair (`fed_party`) and President of the United States (`potus_party`) [6], [7]. These variables are equal to 1 when the position was held by a Republican and 0 when held by a Democrat. Since this data is undisputed and readily accessible from several sources, we used a dataset available on Wikipedia after double-checking its veracity.\n",
    "\n",
    "Our final explanatory feature, `cli`, was pulled from The Organisation for Economic Co-operation and Development (OECD) [8]. This organization is a multinational group founded in 1961 for the purpose of stimulating economic progress and world trade [9]. Given its status as a United Nations observer, we are confident in the validity of its data. The composite leading indicator (CLI) is a statistic created to \"provide early signals of turning points in business cycles\". Hence, the Fed would likely care about proactively adjusting interest rates in response to changes in this indicator.\n",
    "\n",
    "Finally, our data on the FFR (`ffr`) came from the research of an economics professor at Williams University. Kenneth Kuttner is a leading researcher in monetary policy, and the data we downloaded from his website was the backbone of a paper he published in the Journal of Finance (the top journal in academic finance) [10]. It contains information on the percentage point change (`change`) at each announcement and the direction of each change (`direction`). The direction is indicated by -1 for a decrease, 0 for no change, and 1 for an increase in the FFR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Citations:\n",
    "\n",
    "1. https://fred.stlouisfed.org/\n",
    "2. https://en.wikipedia.org/wiki/Chair_of_the_Federal_Reserve\n",
    "3. https://simple.wikipedia.org/wiki/List_of_presidents_of_the_United_States\n",
    "4. https://data.oecd.org/leadind/composite-leading-indicator-cli.htm\n",
    "5. https://en.wikipedia.org/wiki/OECD\n",
    "6. https://econ.williams.edu/faculty-pages/research/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The data is pulled from five primary sources: FRED, Yahoo! Finance, OECD, Wikipedia, and Dr. Kuttner's website. FRED and Yahoo! Finance data is easily pulled using their API by entering a specific data series identifier. The OECD data was manually downloaded as a CSV and loaded. Wikipedia data on political parties was inputted by hand and also loaded from CSV. The FFR data from Dr. Kuttner's research page was downloaded as a CSV and loaded using pandas. The details of this process, including merging the data with different frequences, is demonstrated and explained in the Appendix. Given, our data was pulled from publicly accessible websites, and we thus expect it to be free from any legal problems.\n",
    "\n",
    "The datasets we obtained were well-maintained, containing no missing data or invalid values. However, our data did not agree in frequency. For example, some variables including GDP and exports are reported quarterly, while others such as personal expenditures and the composite leading indicator are reported monthly. Further, a couple features (stock closing price and dollar index) change daily.\n",
    "\n",
    "In addition, we realized that using the current day's values to predict changes in the federal funds interest rate would result in \"look-ahead\" bias. For example, it is unrealistic to believe that the closing stock price at 4:00 PM in New York on a given day would have any effect on an FOMC decision in Washington D.C. made at noon earlier that day. More likely, today's decisions are influenced by data ranging from one to several days prior.\n",
    "\n",
    "To resolve these issues, we first filled in all empty days between reports of monthly and quarterly reported variables with the value from the previous report so we had values for all features for each day. We then used our `collapse()` function, which took our forward filled dataframe of over 12000 rows, shifted the data by a few days (to avoid look-ahead bias), and returned an aggregated dataframe with just 275 rows corresponding to scheduled FOMC announcement days. We exclude unscheduled FOMC announcements because people do not know they are happening until after they have been announced. In addition, nearly every unscheduled FOMC meeting has led to a change in rates, as they are often held for emergency reasons, such as COVID-19. We are focused only on predicting the results of known FOMC meetings, as these are the only meetings anyone can even attempt to predict without a knowledge of the future, so we drop the unscheduled meetings from our dataframe.\n",
    "\n",
    "We have 3 different types of features, and we handled each differently. The first type was categorical data. In our given window (e.g., 60 days prior to the announcement until a week before the announcement) we take the most current data point (seven days before the announcement) and use that datapoint as our categorical feature in our newly generated dataframe. For monthly data, we take the final datapoint in our window (seven days before, just like we did for categorical), but we also add a new delta feature, which is the change from the start of the window. This essentially doubles the number of features we have in our dataset; we now have both a point estimate and its trend for the period. If our dataframe was 12561 rows by 5 monthly features, our new dataframe would be 275 days by 10 features now.\n",
    "\n",
    "Finally, the third type of data changes on a daily basis. We wanted to have a point estimate as well as a trend for this data, so we decided to run a linear regression within each of the windows for each feature. We then take the 275 linear regression models made for each feature and report the corresponding betas (trend) and intercepts (point estimate). All of this data engineering is performed in the `collapse()` function. Our hypothesis is that both point estimate and trend will have an important role to play in our predictions of the rate changes. This is because changes in the target interest rate are often due to changes in economic conditions. Our `collapse()` function aggregates all data in the appropriate window preceding the FOMC meeting, extracting both the point estimate and trend of all indicators, excluding categorical data. Categorical data is excluded because it changes only over longer time horizons.\n",
    "\n",
    "Generating these extra features likely has high autocorrelation, and as such, would likely hurt our model without some sort of dimensionality reduction. We perform Factor Analysis of Mixed Data (FAMD, an analogue to PCA for both categorical and continuous data) in order to only highlight the most important variation of our collapsed dataset. We then perform classification and learning analysis on this collapsed and cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness\n",
    "\n",
    "The data cleaning and feature engineering code found above is flexible and simple to modify. One can easily add not only new features but also recent FOMC decisions. Adding new data series from FRED and Yahoo! Finance is the easiest modification. Simply add the identifying name of the feature as well as the series code for FRED or the ticker for Yahoo! Finance, and the for loops will handle the rest. Adding data from outside CSV files only requires adding the name of the new dataframe to the list when creating `df_features`. Thus, any number of new features (columns) from a variety of sources can be quickly added. By virtue of the `ffill()` function, any data pulled using the FRED or Yahoo! Finance API is properly extended to the last row of the dataframe, which corresponds to the most recent FOMC announcement we have data for.\n",
    "\n",
    "New FOMC announcement data can be appended by modifying the ffr_clean.csv file. This is done by adding the new target Fed Funds Rate under the `ffr` column and the change in basis points under the `change` column. In addition, the `scheduled` column should be entered as 1 if the meeting was scheduled and 0 if it was a surprise. The columns `decision` and `mom` are calculated automatically. It is critical to note that new FOMC announcements should be added sequentially and even if the meeting was unscheduled in order to keep the data consistent. Modifying this data is more of a hassle than adding new features, but meetings generally only occur every six weeks and require the addition of only three data points to the CSV file. All things considered, this is an insignificant burden to adding more data points. In addition, when new data points are added, the data coming from FRED and Yahoo! Finance automatically pull from 1989 until the present day. It is the responsibility of the researcher to ensure any data coming from CSV files is still accurate when adding new FOMC announcement data, as this will not automatically refresh, but autofill from the most recent valid entry using the `ffill()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization and Basic Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below we plot the original dataset and the collapsed dataset in two dimensions using the factorial axes generated by FAMD (the analogue to principal components from PCA) to reduce dimensionality while minimizing information loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![title](df_factor_analysis_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see on the left the data is noisy and contains many small clusters. On the right the data is cleaner while preserving the same basic structure of the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Algorithms and In-Depth Analysis\n",
    "\n",
    "Our primary, naive attempt was to do a XGBoost regression. Accommodating to continuous data required we split our training and testing sets on a specific date. However, as seen below, this failed to properly model the changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recognizing that predicting the exact interest rate would be difficult, we decided classifying an increase, decrease, or no change in the rate rather than regression was a better approach. For our classification problem, we employ the ensamble algorithms XGBoost and Random Forests, as well as other other methods SoftMax Logistic Regression and GDA.\n",
    "\n",
    "Since the beginning of our data, there have been 275 scheduled FOMC decisions. With this limited selection of data, we use Stratified 5-Fold Cross Validation for testing our model, averaging the score across each group. To optimally tune the hyperparameters, we utilize the library `Bayesian Optimization`, which uses bayesian inference and gaussian processes to optimally iterate through hyperparameters. See the appendix for our implementation of this code. After tuning hyperparameters, we test each model, using KFold validation in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy Results:\n",
      "RandomForestClassifier - Average Accuracy: 0.8028282828282828\n",
      "XGBClassifier - Average Accuracy: 0.7920538720538721\n",
      "Softmax Logistic Regression - Average Accuracy: 0.7518518518518519\n",
      "GDA - Average Accuracy: 0.6496969696969697\n",
      "\n",
      "Classification Reports:\n",
      "RandomForestClassifier - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.42      0.55        33\n",
      "         1.0       0.80      0.96      0.87       188\n",
      "         2.0       0.87      0.49      0.63        53\n",
      "\n",
      "    accuracy                           0.80       274\n",
      "   macro avg       0.81      0.62      0.68       274\n",
      "weighted avg       0.81      0.80      0.78       274\n",
      "\n",
      "XGBClassifier - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.39      0.52        33\n",
      "         1.0       0.78      0.96      0.86       188\n",
      "         2.0       0.88      0.43      0.58        53\n",
      "\n",
      "    accuracy                           0.79       274\n",
      "   macro avg       0.81      0.60      0.66       274\n",
      "weighted avg       0.80      0.79      0.77       274\n",
      "\n",
      "Softmax Logistic Regression - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.45      0.56        33\n",
      "         1.0       0.78      0.88      0.83       188\n",
      "         2.0       0.61      0.47      0.53        53\n",
      "\n",
      "    accuracy                           0.75       274\n",
      "   macro avg       0.70      0.60      0.64       274\n",
      "weighted avg       0.74      0.75      0.74       274\n",
      "\n",
      "GDA - Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.44      0.55      0.49        33\n",
      "         1.0       0.81      0.66      0.73       188\n",
      "         2.0       0.45      0.66      0.53        53\n",
      "\n",
      "    accuracy                           0.65       274\n",
      "   macro avg       0.56      0.62      0.58       274\n",
      "weighted avg       0.69      0.65      0.66       274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def categorical(df, col_split):\n",
    "    \"\"\"\n",
    "    Prepares the data for the model by standardizing the continuous features, \n",
    "    converting the categorical features to strings, \n",
    "    and converting sparse columns to a dense format for FAMD.\n",
    "    \"\"\"\n",
    "    categorical_cols = df.columns[:col_split]\n",
    "\n",
    "    # converting the categorical features to strings\n",
    "    df[categorical_cols] = df[categorical_cols].astype(str)\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data(data_address='beta_dates/beta_data_2_42.csv', label_address='beta_dates/true_labels.csv'):\n",
    "    \"\"\"\n",
    "    Loads the data and labels from the specified addresses.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_address, index_col=0)\n",
    "    y = pd.read_csv(label_address, index_col=0).values[:,0]\n",
    "    # add one to the labels to make them 0, 1, 2 instead of -1, 0, 1 (needed for XGBClassifier)\n",
    "    y = y + 1\n",
    "    X = categorical(df, 4)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Function to perform cross-validation and return average accuracy and classification report\n",
    "def evaluate_model(pipe, X, y, params, n_splits=5):\n",
    "    \"\"\"\n",
    "    Performs cross-validation on the given model and returns the average accuracy and classification report.\n",
    "    \"\"\"\n",
    "    pipe.set_params(**params)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    accuracy_scores = []\n",
    "    all_predictions = []\n",
    "    all_true_values = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        pipe.fit(X_train, y_train)\n",
    "        predictions = pipe.predict(X_test)\n",
    "        accuracy_scores.append(accuracy_score(y_test, predictions))\n",
    "        all_predictions.extend(predictions)\n",
    "        all_true_values.extend(y_test)\n",
    "\n",
    "    report = classification_report(all_true_values, all_predictions, zero_division=0)\n",
    "    return np.mean(accuracy_scores), report\n",
    "\n",
    "# Load your data\n",
    "X, y = load_data()\n",
    "\n",
    "# Apply FAMD for dimensionality reduction\n",
    "famd = FAMD(n_components=6)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "# Evaluate RandomForestClassifier\n",
    "rf_params = {'rf__max_depth': 4, 'rf__max_features': 0.65, 'rf__min_samples_leaf': 6, 'rf__min_samples_split': 17, 'rf__n_estimators': 200}\n",
    "avg_rf_accuracy, rf_report = evaluate_model(Pipeline([('rf', RandomForestClassifier())]), X_transformed, y, rf_params)\n",
    "\n",
    "\n",
    "# Evaluate XGBClassifier\n",
    "xgb_params = {'xgb__colsample_bytree': 1.0, 'xgb__gamma': 3.646092407548312, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 6, 'xgb__min_child_weight': 9.488817186497869, 'xgb__n_estimators': 81, 'xgb__reg_alpha': 1.5988455864986395, 'xgb__reg_lambda': 3.267653422896763, 'xgb__subsample': 1.0}\n",
    "avg_xgb_accuracy, xgb_report = evaluate_model(Pipeline([('xgb', XGBClassifier(objective='multi:softmax'))]), X_transformed, y, xgb_params)\n",
    "\n",
    "\n",
    "# Evaluate Softmax Logistic Regression\n",
    "avg_lr_accuracy, lr_report = evaluate_model(Pipeline([('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs'))]), X_transformed, y, params={})\n",
    "\n",
    "\n",
    "# Evaluate GDA\n",
    "avg_gda_accuracy, gda_report = evaluate_model(Pipeline([('gda', QuadraticDiscriminantAnalysis())]), X_transformed, y, params={})\n",
    "\n",
    "print('Average Accuracy Results:')\n",
    "print('RandomForestClassifier - Average Accuracy:', avg_rf_accuracy)\n",
    "print('XGBClassifier - Average Accuracy:', avg_xgb_accuracy)\n",
    "print('Softmax Logistic Regression - Average Accuracy:', avg_lr_accuracy)\n",
    "print('GDA - Average Accuracy:', avg_gda_accuracy)\n",
    "\n",
    "print('\\nClassification Reports:')\n",
    "print('RandomForestClassifier - Classification Report:\\n', rf_report)\n",
    "print('XGBClassifier - Classification Report:\\n', xgb_report)\n",
    "print('Softmax Logistic Regression - Classification Report:\\n', lr_report)\n",
    "print('GDA - Classification Report:\\n', gda_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the Random Forest Classifier emerges as the top-performing model with an average accuracy of approximately 80%. This superior performance can be attributed to the inherent strengths of Random Forests in handling many data types, its robustness to overfitting, and its ability to model complex interactions and non-linear relationships. The classification report shows high precision and recall for class 1.0 (no change in interest rate), indicating strong performance in identifying the most prevalent class. However, it's notable that the recall for rate increases and decreases is comparatively lower, suggesting some challenges in correctly classifying the minority classes.\n",
    "\n",
    "XGBClassifier, with an average accuracy of about 79%, is a close second.  The classification report for XGBoost mirrors that of the Random Forest, with a slight decrease in the recall for interest rate increases and decreases. This indicates a similar trend in the models' ability to handle minority classes.\n",
    "\n",
    "Softmax Logistic Regression shows an average accuracy of around 75%. Its linearity likely provides limitations compared to ensemble methods (Random Forest and XGBoost), which might not capture complex economic decision-making scenarios. The classification report reflects a balanced performance across classes, albeit with lower precision and recall values than the ensemble methods, especially for the minority classes.\n",
    "\n",
    "Gaussian Discriminant Analysis exhibits the lowest average accuracy at approximately 65%. GDA makes assumptions about the distribution of data (i.e., Gaussian), which likely does not hold true for our dataset, possibly leading to its lower performance. Interestingly, the classification report shows a relatively higher recall for the minority classes compared to the other models, indicating its potential in identifying these classes, albeit at the cost of precision.\n",
    "\n",
    "Returning to the fundamental questions of our project:\n",
    "\n",
    "#### What regression and classification methods are best suited to forecasting FOMC decisions?\n",
    "\n",
    "Among the methods analyzed, the RandomForestClassifier is best suited for forecasting these decisions. Its ability to handle complex data structures and its robustness against overfitting make it the most effective choice for this classification task.\n",
    "\n",
    "#### How accurately can we predict FOMC decisions?\n",
    "\n",
    "The RandomForestClassifier, as the most effective model in our analysis, predicts FOMC decisions with an average accuracy of approximately 80%. This level of accuracy indicates a high degree of reliability in the model's predictions. However, it's important to note that while this accuracy is notable, there remains a degree of uncertainty, particularly in predicting less frequent decision outcomes. Therefore, while predictions are generally reliable, they should be contextualized within the inherent unpredictability of FOMC decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Implications\n",
    "\n",
    "We have attempted to develop a model for predicting changes in the federal funds rate based on available economic indicators. As mentioned previously, the ability to predict changes to this prevailing interest rate before FOMC announcement allows for making strategical decisions to minimize or eliminate losses. For example, a prior prediction of what will happen to interest rates might motivate an individual to move from a low-security job to a high-security job, choose to wait or to rush to get a loan, or sell and buy stocks prior to a big move in the stock market.\n",
    "\n",
    "Our intent and methodology raise a number of ethical issues. First, attempting to obtain an advantage through  information about the future inherently leads to others who do not share that information facing a disadvantage. For example, someone who has highly credible evidence to believe that the Federal Reserve will decrease the federal funds rate has incentive to sell stocks while keeping this information private so that others do not push the stock price down. Yet by not sharing that evidence, such an individual puts other investors in a more difficult place. Such moral grounds are the basis for material nonpublic information laws that prohibit using, sharing, or selling information about future company decisions that will affect stock prices to turn a profit (#A). While the information obtained through analysis such as that performed in this portfolio is not nearly as certain or exclusive (as the work we have done is highly reproducible), it is still important to acknowledge that successful attempts to predict the market in order to profit will inevitably result in someone losing in a zero-sum, competitive financial world.\n",
    "\n",
    "Another ethical consideration is the potential loss that could be incurred when too much trust is placed in the prediction of the change in the federal fund rate generated by a model such as ours. With so many factors being weighed and considered by the Federal Reserve, and with economic indicators being so numerous and complex, it is inconceivable that any model used to predict changes in the federal funds rate could have near-perfect accuracy. Those that do not understand the extreme uncertainty involved in both the inputs and outputs to the model are likely to pivot too far, opening themselves up to loss incurred when the actual Federal Reserve decision flies in the face of predictions. In making and sharing predictions, therefore, it is important that equal emphasis and care is placed to express the uncertainty of the outcome, as well as to acknowledge that despite warnings, many will take risks based on the predictions that may have severe economic consequences.\n",
    "\n",
    "Finally, it is important to note that our aim and methodology of predicting economic outcomes, when implemented and replicated on a large scale, may actually influence the outcomes themselves. Several times throughout history, the mere expectation of economic outcomes or a lack of faith in economic indicators have created economic disasters. For instance, in the events of Black Monday on October 19, 1987, the Dow Jones Industrial Average fell twenty percent in one day. The initial trigger was nervousness about possible overvaluation of stocks and the decline of the dollar, but the crisis was intensified by panic due to a lack of information as to what was happening (\"B\"). This episode and many others demonstrate that expectations or predictions of what will happen, whether grounded or totally unfounded, can often become a self-fulfilling prophecy. Thus it is possible that initially spurious relationships between economic indicators and interest rates mistakenly included or even emphasized in a model like this one might actually generate an unintended (and perhaps undesired) causal relationship, especially when machine learning is involved. Thus care must be taken to critically analyze how features are included, removed, or split on in such models, especially when destructive self-fulfilling feedback loops can have dramatic financial effects on millions across the country.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations:\n",
    "\n",
    "[1] “Monetary Policy: What Are Its Goals? How Does It Work?” Federal Reserve Board - Monetary Policy: What Are Its Goals? How Does It Work?, www.federalreserve.gov/monetarypolicy/monetary-policy-what-are-its-goals-how-does-it-work.htm.\n",
    "\n",
    "[2] Dziubinski, Susan. “How to Invest Your Money as the Fed Raises Interest Rates.” Morningstar, Inc., 27 July 2022, www.morningstar.com/portfolios/how-invest-your-money-fed-raises-interest-rates. \n",
    "\n",
    "[3] “Federal Open Market Committee.” The Fed - Federal Open Market Committee, www.federalreserve.gov/monetarypolicy/fomc.htm.\n",
    "\n",
    "[4] Gavin, William T, and Rachel J Mandal. “The Art of Predicting the Federal Reserve: St. Louis Fed.” Saint Louis Fed Eagle, Federal Reserve Bank of St. Louis, 9 Dec. 2021, www.stlouisfed.org/publications/regional-economist/july-2000/inside-the-briefcase-the-art-of-predicting-the-federal-reserve.\n",
    "\n",
    "[5] Chen, James. “Material Nonpublic Information (MNPI)? Definition and Laws.” Investopedia, Investopedia, www.investopedia.com/terms/m/materialinsiderinformation.asp.\n",
    "\n",
    "[6] Carlson, Mark A. “A brief history of the 1987 stock market crash with a discussion of the Federal Reserve Response.” Finance and Economics Discussion Series, vol. 2006, no. 13, 2006, pp. 1–24, https://doi.org/10.17016/feds.2007.13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "The appendix contains both our code to pull the data pull and code to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Pulls\n",
    "\n",
    "# specify what data to pull from FRED\n",
    "fred = fd.Fred(api_key = 'bde7928ce1d3cc555b5d2fb725f0ef4b')\n",
    "start_dt = '1/1/1989'\n",
    "name_id = [('loan', 'PRIME'), ('exports', 'A020RL1Q158SBEA'), ('pce', 'PCETRIM12M159SFRBDAL'),\n",
    "           ('ue', 'UNRATE'), ('rgdp', 'A191RL1Q225SBEA'), ('cars', 'TOTALSA'), ('recess', 'USREC'),\n",
    "           ('gdpd', 'A191RI1Q225SBEA'), ('veloc', 'M1V'), ('house', 'HOUST'), ('mich', 'MICH')]\n",
    "fred_data = []\n",
    "\n",
    "for x in name_id:\n",
    "    # get FRED series using the proper id\n",
    "    df = pd.DataFrame(fred.get_series(x[1], observation_start=start_dt))\n",
    "    # make column name identifiable\n",
    "    df.rename(columns={0: x[0]}, inplace=True)\n",
    "    fred_data.append(df)\n",
    "\n",
    "# aggregate FRED data series\n",
    "df_fred = pd.concat(fred_data, axis=1)\n",
    "\n",
    "# specify what data to pull from Yahoo! Finance\n",
    "tickers = [('spx', '^SPX'), ('usd', 'DX-Y.NYB')]\n",
    "start_dt = '1989-01-01'\n",
    "end_dt = str(date.today() - timedelta(1))\n",
    "interval = '1d'\n",
    "stock_data = []\n",
    "\n",
    "# access data and display\n",
    "for t in tickers:\n",
    "    # get price data using ticker\n",
    "    df = yf.download(t[1], start = start_dt, end = end_dt, interval= interval)\n",
    "    # make column name identifiable\n",
    "    df = df[['Adj Close']].rename(columns={'Adj Close': t[0]})\n",
    "    stock_data.append(df)\n",
    "\n",
    "# aggregate stock data\n",
    "df_stock = pd.concat(stock_data, axis=1)\n",
    "\n",
    "# read in cli data from csv file downloaded off OECD website\n",
    "df_cli = pd.read_csv('cli.csv', index_col=0)\n",
    "df_cli.index = pd.to_datetime(df_cli.index)\n",
    "\n",
    "# read in manually gathered political party data\n",
    "df_party = pd.read_csv('party.csv', index_col=0)\n",
    "df_party.index = pd.to_datetime(df_party.index)\n",
    "\n",
    "# create feature dataset\n",
    "df_features = pd.concat([df_fred, df_stock, df_cli, df_party], axis=1)\n",
    "\n",
    "# pull in FOMC data \n",
    "df_fomc = pd.read_csv('ffr_clean.csv', index_col=0)\n",
    "df_fomc.index = pd.to_datetime(df_fomc.index)\n",
    "\n",
    "# create decisions and momentum column\n",
    "decision = df_fomc['change'].values.copy()\n",
    "down, up = tuple([decision < 0]), tuple([decision > 0])\n",
    "decision[down], decision[up] = -1., 1.\n",
    "\n",
    "# create new columns\n",
    "df_fomc['decision'] = decision\n",
    "df_fomc['mom'] = decision\n",
    "\n",
    "# consider only scheduled meetings\n",
    "df_fomc = df_fomc.query('scheduled == 1')\n",
    "df_fomc.drop('scheduled', axis=1, inplace=True)\n",
    "\n",
    "# get start and end dates\n",
    "start_dt = df_features.index[0]\n",
    "end_dt = df_fomc.index[-1]\n",
    "\n",
    "# get every day between start and end date\n",
    "delta = end_dt - start_dt\n",
    "daily = [start_dt + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "\n",
    "# make dataframe of dates\n",
    "df_days = pd.DataFrame(daily)\n",
    "df_days.index = pd.to_datetime(df_days.values[:,0])\n",
    "\n",
    "# initialize master dataframe\n",
    "df = pd.concat([df_days, df_fomc, df_features], axis=1)\n",
    "\n",
    "# drop irrelevant columns and rows\n",
    "df.drop(0, axis=1, inplace=True)\n",
    "last_ix = df_days.index.get_loc(end_dt) + 1\n",
    "df.drop(df.index[last_ix:], inplace=True)\n",
    "\n",
    "# find start and stop columns\n",
    "cols = df.columns.to_list()\n",
    "start_ix, end_ix = cols.index('mom'), cols.index('fed_party')\n",
    "\n",
    "# shift necessary columns\n",
    "df_shift = df.iloc[:, start_ix:end_ix].copy()\n",
    "df.iloc[:, start_ix:end_ix] = df_shift.shift()\n",
    "\n",
    "# fill in NaN entries\n",
    "df_fill = df.iloc[:, start_ix:].copy()\n",
    "df.iloc[:, start_ix:] = df_fill.ffill()\n",
    "\n",
    "# save master dataframe\n",
    "df.to_csv('master_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Cleaning\n",
    "\n",
    "# define the columns\n",
    "monthly = ['pce','ue','cars','house','cli','exports','rgdp','gdpd','veloc', 'ffr','mich']\n",
    "labels = ['decision', 'ffr', 'change']\n",
    "daily = ['spx','usd', 'loan']\n",
    "categorical = ['fed_party','potus_party','recess', 'mom']\n",
    "\n",
    "# define the function to collapse the data to the event days\n",
    "def collapse(df, key = 'label_day', start = 7, end = 60, cat_col = [], daily_col = [], other = [], labels = []):\n",
    "    \"\"\"A function to collapse the data into a single row for each event, capturing trends and point in time data.\n",
    "    df: dataframe\n",
    "    key: column name of the event day\n",
    "    start: number of days before the event to capture\n",
    "    end: number of days after the event to capture\"\"\"\n",
    "    \n",
    "    # Get our different dataframes\n",
    "    categorical = df[cat_col]\n",
    "    daily = df[daily_col]\n",
    "    monthly = df[other]\n",
    "    keep = df[key]\n",
    "    labels = df[labels]\n",
    "\n",
    "    # create the windows\n",
    "    event_days = df[df[key]].index\n",
    "\n",
    "    ################## Handle Categorical and Label Data ##################\n",
    "    cat_collapsed = categorical[keep]\n",
    "    y_labels = labels[keep]\n",
    "\n",
    "    ################## Handle Monthly Data ##################\n",
    "    # Create event windows\n",
    "    windows = []\n",
    "    for day in event_days:\n",
    "        # Calculate start and end days of the window\n",
    "        start_day = day - pd.Timedelta(days=start)\n",
    "        end_day = day - pd.Timedelta(days=end)\n",
    "\n",
    "        # Ensure start_day is not before the start of the dataset\n",
    "        if end_day < df.index[0]:\n",
    "            end_day = df.index[0]\n",
    "\n",
    "        windows.append((start_day, end_day))\n",
    "\n",
    "    # Initialize a list to store aggregated data\n",
    "    aggregated_data = []\n",
    "\n",
    "    for (start_day, end_day) in windows:\n",
    "        # Select data within the window\n",
    "        window_data = monthly.loc[end_day:start_day]\n",
    "\n",
    "        # Apply your custom aggregation function\n",
    "        final_vals = window_data.loc[start_day]\n",
    "        change = final_vals - window_data.iloc[0]\n",
    "        # relabel the change columns to include a d_ prefix\n",
    "        change = change.rename(lambda x: 'd_' + x)\n",
    "\n",
    "        # concatenate the data horizontally\n",
    "        aggregated_window = pd.concat([final_vals, change], axis=0)\n",
    "        aggregated_data.append(aggregated_window)\n",
    "\n",
    "    # Combine aggregated data\n",
    "    aggregated_monthly = pd.concat(aggregated_data, axis=1).T\n",
    "    aggregated_monthly.index = event_days\n",
    "\n",
    "    ################## Handle Daily Data ##################\n",
    "    # define the model\n",
    "    model = linear_model.LinearRegression()\n",
    "    daily_agg = []\n",
    "\n",
    "    # Get the names of the features\n",
    "    daily_feat = daily.columns\n",
    "    b0_name = ['b0_' + feat for feat in daily_feat]\n",
    "    b1_name = ['b1_' + feat for feat in daily_feat]\n",
    "\n",
    "    for (start_day, end_day) in windows:\n",
    "        # Select data within the window\n",
    "        window_data = daily.loc[end_day:start_day]\n",
    "        time = np.arange(len(window_data)).reshape(-1, 1)\n",
    "\n",
    "        # Initialize lists to store betas for each feature\n",
    "        beta0 = []\n",
    "        beta1 = []\n",
    "\n",
    "        # Loop through each feature and fit the model\n",
    "        for feat in daily_feat:\n",
    "            model.fit(time, window_data[feat])\n",
    "            beta0.append(model.intercept_)\n",
    "            beta1.append(model.coef_[0])\n",
    "\n",
    "        # Create DataFrames for betas\n",
    "        beta0_df = pd.DataFrame([beta0], columns=b0_name)\n",
    "        beta1_df = pd.DataFrame([beta1], columns=b1_name)\n",
    "\n",
    "        # concatenate the data horizontally\n",
    "        aggregated_window = pd.concat([beta0_df, beta1_df], axis=1)\n",
    "        daily_agg.append(aggregated_window)\n",
    "\n",
    "    # Combine aggregated data\n",
    "    aggregated_daily = pd.concat(daily_agg, axis=0)\n",
    "    aggregated_daily.index = event_days\n",
    "\n",
    "    ################## Combine Data ##################\n",
    "    # Combine all data\n",
    "    combined = pd.concat([cat_collapsed, aggregated_monthly, aggregated_daily], axis=1)\n",
    "    return combined, y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "X, y = load_data()\n",
    "\n",
    "# Apply FAMD for dimensionality reduction\n",
    "famd = FAMD(n_components=13)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "# Create a pipeline with Random Forest\n",
    "pipe = Pipeline([\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Function to optimize with cross-validation\n",
    "def rf_evaluate(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features):\n",
    "    params = {\n",
    "        'rf__n_estimators': int(n_estimators),\n",
    "        'rf__max_depth': int(max_depth),\n",
    "        'rf__min_samples_split': int(min_samples_split),\n",
    "        'rf__min_samples_leaf': int(min_samples_leaf),\n",
    "        'rf__max_features': max_features,\n",
    "    }\n",
    "    pipe.set_params(**params)\n",
    "    return np.mean(cross_val_score(pipe, X_transformed, y, cv=5, scoring='accuracy'))\n",
    "\n",
    "# Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=rf_evaluate,\n",
    "    pbounds={\n",
    "        'n_estimators': (50, 300),\n",
    "        'max_depth': (3, 5),\n",
    "        'min_samples_split': (2, 30),\n",
    "        'min_samples_leaf': (1, 30),\n",
    "        'max_features': (0.1, 1)\n",
    "    },\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=5, n_iter=200)\n",
    "\n",
    "# After optimization, retrieve and adjust best parameters for pipeline\n",
    "rf_best_params = optimizer.max['params']\n",
    "rf_best_params_for_pipeline = {'rf__' + key: int(value) if key in ['max_depth', 'n_estimators', 'min_samples_split', 'min_samples_leaf'] else value for key, value in rf_best_params.items()}\n",
    "\n",
    "# Set the best parameters in the pipeline\n",
    "pipe.set_params(**rf_best_params_for_pipeline)\n",
    "pipe.fit(X_transformed, y)\n",
    "\n",
    "# Print best parameters and performance\n",
    "print(\"Best Parameters for Pipeline:\", rf_best_params_for_pipeline)\n",
    "print(\"Best Performance:\", optimizer.max['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "X, y = load_data()\n",
    "\n",
    "# Apply FAMD for dimensionality reduction\n",
    "famd = FAMD(n_components=13)\n",
    "famd.fit(X)\n",
    "X_transformed = famd.transform(X)\n",
    "\n",
    "# Create a pipeline with XGBoost\n",
    "pipe = Pipeline([\n",
    "    ('xgb', XGBClassifier(objective='multi:softprob'))  # Using multi:softprob for multi-class classification\n",
    "])\n",
    "\n",
    "# Function to optimize with cross-validation\n",
    "def xgb_evaluate(n_estimators, max_depth, learning_rate, gamma, min_child_weight, subsample, colsample_bytree, reg_alpha, reg_lambda):\n",
    "    params = {\n",
    "        'xgb__n_estimators': int(n_estimators),\n",
    "        'xgb__max_depth': int(max_depth),\n",
    "        'xgb__learning_rate': learning_rate,\n",
    "        'xgb__gamma': gamma,\n",
    "        'xgb__min_child_weight': min_child_weight,\n",
    "        'xgb__subsample': subsample,\n",
    "        'xgb__colsample_bytree': colsample_bytree,\n",
    "        'xgb__reg_alpha': reg_alpha,\n",
    "        'xgb__reg_lambda': reg_lambda\n",
    "    }\n",
    "    pipe.set_params(**params)\n",
    "    return np.mean(cross_val_score(pipe, X_transformed, y, cv=5, scoring='accuracy'))\n",
    "\n",
    "# Bayesian Optimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=xgb_evaluate,\n",
    "    pbounds={\n",
    "        'n_estimators': (50, 300),\n",
    "        'max_depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'gamma': (0, 5),\n",
    "        'min_child_weight': (1, 10),\n",
    "        'subsample': (0.5, 1),\n",
    "        'colsample_bytree': (0.5, 1),\n",
    "        'reg_alpha': (0, 5),\n",
    "        'reg_lambda': (0, 5)\n",
    "    },\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=5, n_iter=200)\n",
    "\n",
    "# After optimization, retrieve and adjust best parameters for pipeline\n",
    "xgb_best_params = optimizer.max['params']\n",
    "xgb_best_params_for_pipeline = {'xgb__' + key: int(value) if key in ['n_estimators', 'max_depth'] else value for key, value in xgb_best_params.items()}\n",
    "\n",
    "# Set the best parameters in the pipeline\n",
    "pipe.set_params(**xgb_best_params_for_pipeline)\n",
    "pipe.fit(X_transformed, y)\n",
    "\n",
    "# Print best parameters and performance\n",
    "print(\"Best Parameters for Pipeline:\", xgb_best_params_for_pipeline)\n",
    "print(\"Best Performance:\", optimizer.max['target'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
